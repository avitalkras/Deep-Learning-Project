{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h8XmJVfP-A8"
      },
      "source": [
        "# Part A - Van Gogh Painting Classifier\n",
        "\n",
        "## Deep Learning Project - Tel Aviv University\n",
        "\n",
        "This notebook implements a binary classifier to identify Van Gogh paintings using transfer learning with VGG19.\n",
        "\n",
        "### Requirements:\n",
        "- Google Colab with GPU runtime\n",
        "- Kaggle API token (set in notebook)\n",
        "- Weights & Biases account for experiment tracking\n",
        "- Git repository connection (notebook opened from GitHub)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iQGUIlOP-A-"
      },
      "source": [
        "## 1. Environment Setup & Kaggle Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DQqOcR7cP-A_",
        "outputId": "9d920786-7eeb-469c-8298-7ab7119792f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Running on Google Colab\n",
            "‚úì Connected via Git repository\n"
          ]
        }
      ],
      "source": [
        "# Check if running on Google Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"‚úì Running on Google Colab\")\n",
        "    print(\"‚úì Connected via Git repository\")\n",
        "else:\n",
        "    print(\"Running locally\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "T-wFi03lP-BA"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q kaggle optuna wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Eps1cNyuP-BB",
        "outputId": "0591aa05-a57c-474d-89f8-06897b8ad926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Kaggle API configured successfully\n",
            "  Username: avitalkrasovitskii\n",
            "  Key: 53241c7887...\n"
          ]
        }
      ],
      "source": [
        "# Setup Kaggle API\n",
        "# The Kaggle CLI requires a kaggle.json file with username and key\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Extract key from token (format: KGAT_<key>)\n",
        "KAGGLE_TOKEN = 'KGAT_53241c788774fa128a90f2ff25802746'\n",
        "KAGGLE_KEY = KAGGLE_TOKEN.replace('KGAT_', '')  # Extract the key part\n",
        "KAGGLE_USERNAME = 'avitalkrasovitskii'  # Update this if your Kaggle username is different\n",
        "\n",
        "# Create .kaggle directory\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "\n",
        "# Create kaggle.json file\n",
        "kaggle_config = {\n",
        "    \"username\": KAGGLE_USERNAME,\n",
        "    \"key\": KAGGLE_KEY\n",
        "}\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
        "    json.dump(kaggle_config, f)\n",
        "\n",
        "# Set proper permissions\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
        "\n",
        "# Also set environment variable for compatibility\n",
        "os.environ['KAGGLE_USERNAME'] = KAGGLE_USERNAME\n",
        "os.environ['KAGGLE_KEY'] = KAGGLE_KEY\n",
        "\n",
        "print(\"‚úì Kaggle API configured successfully\")\n",
        "print(f\"  Username: {KAGGLE_USERNAME}\")\n",
        "print(f\"  Key: {KAGGLE_KEY[:10]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le-VZOv-P-BB",
        "outputId": "c32fe519-e359-4437-9f75-442f85264ef4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/steubk/wikiart\n",
            "License(s): CC0-1.0\n"
          ]
        }
      ],
      "source": [
        "# Download WikiArt Dataset from Kaggle and Filter to Post_Impressionism Only\n",
        "# Dataset: https://www.kaggle.com/datasets/steubk/wikiart\n",
        "# Warning: Full dataset is ~32GB. We will download it and filter to only Post_Impressionism folder.\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Check if running on Kaggle (dataset already available)\n",
        "kaggle_path = \"/kaggle/input/wikiart/Post_Impressionism\"\n",
        "if os.path.exists(kaggle_path):\n",
        "    print(\"‚úì Running on Kaggle - dataset already available!\")\n",
        "    print(f\"‚úì Using dataset at: {kaggle_path}\")\n",
        "    files = [f for f in os.listdir(kaggle_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    print(f\"‚úì Found {len(files)} images in Post_Impressionism folder\")\n",
        "else:\n",
        "    # Download and filter dataset (for Colab or local)\n",
        "    print(\"üì• Downloading WikiArt dataset from Kaggle...\")\n",
        "    print(\"‚ö†Ô∏è  WARNING: Full dataset is ~32GB. Download may take 30-60 minutes.\")\n",
        "    print(\"    We will filter to only Post_Impressionism folder to save space.\\n\")\n",
        "    \n",
        "    # Set paths\n",
        "    data_root = \"/content/data\"\n",
        "    target_folder = os.path.join(data_root, \"Post_Impressionism\")\n",
        "    temp_dir = os.path.join(data_root, \"temp_extract\")\n",
        "    \n",
        "    # Create directories\n",
        "    os.makedirs(data_root, exist_ok=True)\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "    \n",
        "    # Check if already downloaded\n",
        "    if os.path.exists(target_folder) and len(os.listdir(target_folder)) > 0:\n",
        "        files = [f for f in os.listdir(target_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        print(f\"‚úì Post_Impressionism folder already exists with {len(files)} images\")\n",
        "        print(f\"  Location: {target_folder}\")\n",
        "    else:\n",
        "        # Step 1: Download the full dataset\n",
        "        print(\"\\n[Step 1/3] Downloading full dataset...\")\n",
        "        try:\n",
        "            !kaggle datasets download -d steubk/wikiart -p /content/data\n",
        "            print(\"‚úì Download completed!\")\n",
        "            \n",
        "            # Find the downloaded zip file\n",
        "            zip_files = [f for f in os.listdir(data_root) if f.endswith('.zip')]\n",
        "            if not zip_files:\n",
        "                raise FileNotFoundError(\"No zip file found after download\")\n",
        "            \n",
        "            zip_path = os.path.join(data_root, zip_files[0])\n",
        "            print(f\"  Found zip file: {zip_files[0]}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Download error: {e}\")\n",
        "            raise\n",
        "        \n",
        "        # Step 2: Extract ONLY Post_Impressionism folder\n",
        "        print(\"\\n[Step 2/3] Extracting Post_Impressionism folder from zip...\")\n",
        "        print(\"    This may take 5-10 minutes...\")\n",
        "        \n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                # Get list of all files in zip\n",
        "                all_files = zip_ref.namelist()\n",
        "                \n",
        "                # Filter for Post_Impressionism files only\n",
        "                post_imp_files = [f for f in all_files if 'Post_Impressionism' in f]\n",
        "                \n",
        "                if not post_imp_files:\n",
        "                    raise ValueError(\"Post_Impressionism folder not found in dataset!\")\n",
        "                \n",
        "                print(f\"  Found {len(post_imp_files)} files in Post_Impressionism folder\")\n",
        "                \n",
        "                # Extract only Post_Impressionism files\n",
        "                for file in tqdm(post_imp_files, desc=\"Extracting\"):\n",
        "                    zip_ref.extract(file, temp_dir)\n",
        "                \n",
        "                print(\"  ‚úì Extraction completed!\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Extraction error: {e}\")\n",
        "            raise\n",
        "        \n",
        "        # Step 3: Move Post_Impressionism folder to final location\n",
        "        print(\"\\n[Step 3/3] Organizing files...\")\n",
        "        \n",
        "        # Find the Post_Impressionism folder in temp directory\n",
        "        def find_post_imp_folder(root_dir):\n",
        "            for root, dirs, files in os.walk(root_dir):\n",
        "                if 'Post_Impressionism' in dirs:\n",
        "                    return os.path.join(root, 'Post_Impressionism')\n",
        "            return None\n",
        "        \n",
        "        extracted_path = find_post_imp_folder(temp_dir)\n",
        "        \n",
        "        if extracted_path and os.path.exists(extracted_path):\n",
        "            # Move to final location\n",
        "            if os.path.exists(target_folder):\n",
        "                shutil.rmtree(target_folder)\n",
        "            shutil.move(extracted_path, target_folder)\n",
        "            print(f\"  ‚úì Moved to: {target_folder}\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\"Could not find Post_Impressionism folder after extraction\")\n",
        "        \n",
        "        # Clean up: remove zip file and temp directory\n",
        "        print(\"\\nüßπ Cleaning up temporary files...\")\n",
        "        try:\n",
        "            if os.path.exists(zip_path):\n",
        "                os.remove(zip_path)\n",
        "                print(\"  ‚úì Removed zip file (saved ~30GB)\")\n",
        "            if os.path.exists(temp_dir):\n",
        "                shutil.rmtree(temp_dir)\n",
        "                print(\"  ‚úì Removed temporary extraction folder\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Cleanup warning: {e}\")\n",
        "        \n",
        "        # Verify final result\n",
        "        print(\"\\nüìä Final verification:\")\n",
        "        if os.path.exists(target_folder):\n",
        "            files = [f for f in os.listdir(target_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "            print(f\"‚úì Post_Impressionism folder ready at: {target_folder}\")\n",
        "            print(f\"‚úì Contains {len(files)} image files\")\n",
        "            \n",
        "            # Calculate folder size\n",
        "            total_size = sum(os.path.getsize(os.path.join(target_folder, f)) \n",
        "                             for f in os.listdir(target_folder) \n",
        "                             if os.path.isfile(os.path.join(target_folder, f)))\n",
        "            size_gb = total_size / (1024**3)\n",
        "            print(f\"‚úì Total size: {size_gb:.2f} GB\")\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"Post_Impressionism folder not found at {target_folder}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj0h6Pw0P-BC"
      },
      "source": [
        "## 2. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8aoyWL1P-BF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    roc_curve\n",
        ")\n",
        "\n",
        "# Hyperparameter tuning and logging\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NTjxVeHP-BO"
      },
      "outputs": [],
      "source": [
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device.type == 'cpu':\n",
        "    print(\"‚ö†Ô∏è WARNING: Running on CPU. For faster training, enable GPU in Colab:\")\n",
        "    print(\"   Runtime -> Change runtime type -> Hardware accelerator -> GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96eF9IOzP-BP"
      },
      "outputs": [],
      "source": [
        "# Login to Weights & Biases\n",
        "# Get your API key from: https://wandb.ai/authorize\n",
        "\n",
        "wandb.login()\n",
        "print(\"‚úì Logged in to Weights & Biases\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PylwidTjP-BP"
      },
      "source": [
        "## 3. Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U05M5vt3P-BR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Check for dataset in Kaggle first, then Colab download location\n",
        "kaggle_path = \"/kaggle/input/wikiart/Post_Impressionism\"\n",
        "colab_path = \"/content/data/Post_Impressionism\"\n",
        "\n",
        "if os.path.exists(kaggle_path):\n",
        "    base_dir = kaggle_path\n",
        "    print(f\"‚úì Using Kaggle dataset: {base_dir}\")\n",
        "elif os.path.exists(colab_path):\n",
        "    base_dir = colab_path\n",
        "    print(f\"‚úì Using downloaded dataset: {base_dir}\")\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        f\"‚ùå Post_Impressionism folder not found!\\n\"\n",
        "        f\"  Checked: {kaggle_path}\\n\"\n",
        "        f\"  Checked: {colab_path}\\n\"\n",
        "        f\"Please run the download cell first!\"\n",
        "    )\n",
        "\n",
        "records = []\n",
        "\n",
        "for fname in os.listdir(base_dir):\n",
        "    if not fname.lower().endswith((\".jpg\", \".png\")):\n",
        "        continue\n",
        "\n",
        "    artist = fname.split(\"_\")[0] #takes the first item of the split- the name of artist\n",
        "\n",
        "    records.append({\n",
        "        \"filepath\": os.path.join(base_dir, fname),\n",
        "        \"artist\": artist,\n",
        "        \"is_van_gogh\": int(\"van-gogh\" in artist.lower())\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "print(f\"\\n‚úì DataFrame created with {len(df)} images\")\n",
        "print(f\"  Van Gogh: {df['is_van_gogh'].sum()} images\")\n",
        "print(f\"  Other: {len(df) - df['is_van_gogh'].sum()} images\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLShT63aP-BR"
      },
      "outputs": [],
      "source": [
        "# Split dataset: 70% train, 15% validation, 15% test (stratified)\n",
        "\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, test_size=0.3, stratify=df[\"is_van_gogh\"], random_state=SEED\n",
        ")\n",
        "\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.5, stratify=temp_df[\"is_van_gogh\"], random_state=SEED\n",
        ")\n",
        "\n",
        "print(\"Dataset splits:\")\n",
        "print(f\"  Train: {len(train_df):5d} images ({len(train_df)/len(df):.1%})\")\n",
        "print(f\"  Val:   {len(val_df):5d} images ({len(val_df)/len(df):.1%})\")\n",
        "print(f\"  Test:  {len(test_df):5d} images ({len(test_df)/len(df):.1%})\")\n",
        "\n",
        "print(\"\\nClass distribution per split:\")\n",
        "print(f\"  Train - Van Gogh: {train_df['is_van_gogh'].mean():.2%}\")\n",
        "print(f\"  Val   - Van Gogh: {val_df['is_van_gogh'].mean():.2%}\")\n",
        "print(f\"  Test  - Van Gogh: {test_df['is_van_gogh'].mean():.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwqo_KbRP-BS"
      },
      "source": [
        "## 4. Data Transforms & Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSzBqFT7P-BS"
      },
      "outputs": [],
      "source": [
        "# Define transforms (ImageNet normalization for VGG19)\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Training transforms with data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "])\n",
        "\n",
        "# Evaluation transforms (no augmentation)\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "])\n",
        "\n",
        "print(\"Transforms defined ‚úì\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXFfgteAP-BS"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset class\n",
        "class VanGoghDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image = Image.open(row[\"filepath\"]).convert(\"RGB\")\n",
        "        label = row[\"is_van_gogh\"]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = VanGoghDataset(train_df, transform=train_transform)\n",
        "val_dataset = VanGoghDataset(val_df, transform=eval_transform)\n",
        "test_dataset = VanGoghDataset(test_df, transform=eval_transform)\n",
        "\n",
        "print(f\"Datasets created:\")\n",
        "print(f\"  Train: {len(train_dataset)} samples\")\n",
        "print(f\"  Val:   {len(val_dataset)} samples\")\n",
        "print(f\"  Test:  {len(test_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bccCmJABP-BT"
      },
      "source": [
        "## 5. Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk0j02iOP-BT"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    \"\"\"Train model for one epoch. Returns average loss.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if wandb.run is not None:\n",
        "            wandb.log({\"batch_loss\": loss.item()})\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(model, loader, criterion, device):\n",
        "    \"\"\"Evaluate model. Returns average loss and accuracy.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return total_loss / len(loader), correct / total\n",
        "\n",
        "print(\"Training functions defined ‚úì\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeGOP6L1P-BT"
      },
      "source": [
        "## 6. Hyperparameter Tuning with Optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoopVVlgP-BT"
      },
      "outputs": [],
      "source": [
        "def create_model(freeze_features=True):\n",
        "    \"\"\"Create VGG19 model with modified classifier for binary classification.\"\"\"\n",
        "    model = models.vgg19(weights='IMAGENET1K_V1')\n",
        "\n",
        "    if freeze_features:\n",
        "        for param in model.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, 2)\n",
        "    return model.to(device)\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"Optuna objective function for hyperparameter optimization.\"\"\"\n",
        "    # Suggest hyperparameters\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"AdamW\"])\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
        "    momentum = trial.suggest_float(\"momentum\", 0.8, 0.99) if optimizer_name == \"SGD\" else 0.0\n",
        "    freeze_features = trial.suggest_categorical(\"freeze_features\", [True, False])\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(\n",
        "        project=\"VanGogh_Classifier\",\n",
        "        name=f\"trial_{trial.number}\",\n",
        "        config={\"lr\": lr, \"optimizer\": optimizer_name, \"batch_size\": batch_size,\n",
        "                \"weight_decay\": weight_decay, \"momentum\": momentum,\n",
        "                \"freeze_features\": freeze_features, \"model\": \"VGG19\"},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # Create model and optimizer\n",
        "    model = create_model(freeze_features=freeze_features)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "\n",
        "    if optimizer_name == \"SGD\":\n",
        "        optimizer = torch.optim.SGD(trainable_params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    elif optimizer_name == \"AdamW\":\n",
        "        optimizer = torch.optim.AdamW(trainable_params, lr=lr, weight_decay=weight_decay)\n",
        "    else:\n",
        "        optimizer = torch.optim.Adam(trainable_params, lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 10\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        val_loss, val_acc = eval_one_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "        best_val_acc = max(best_val_acc, val_acc)\n",
        "\n",
        "        wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss,\n",
        "                   \"val_acc\": val_acc, \"best_val_acc\": best_val_acc})\n",
        "\n",
        "        trial.report(val_acc, epoch)\n",
        "        if trial.should_prune():\n",
        "            run.finish()\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    run.finish()\n",
        "    return best_val_acc\n",
        "\n",
        "print(\"Optuna objective function defined ‚úì\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T2jcKusP-BU"
      },
      "outputs": [],
      "source": [
        "# Run hyperparameter search\n",
        "print(\"Starting hyperparameter search...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=2)\n",
        ")\n",
        "\n",
        "# Adjust n_trials and timeout based on your time constraints\n",
        "study.optimize(objective, n_trials=15, timeout=3600, show_progress_bar=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HYPERPARAMETER SEARCH COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nBest validation accuracy: {study.best_value:.4f}\")\n",
        "print(f\"\\nBest hyperparameters:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VzTqTUXP-BU"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM7K6ZYQP-BV"
      },
      "source": [
        "## 7. Train Final Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y6f0EkbP-BV"
      },
      "outputs": [],
      "source": [
        "# Get best parameters and train final model\n",
        "best_params = study.best_params\n",
        "print(\"Training final model with best parameters:\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# Create DataLoaders\n",
        "final_train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True, num_workers=2)\n",
        "final_val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False, num_workers=2)\n",
        "\n",
        "# Create model\n",
        "final_model = create_model(freeze_features=best_params.get('freeze_features', False))\n",
        "\n",
        "# Setup optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "trainable_params = filter(lambda p: p.requires_grad, final_model.parameters())\n",
        "\n",
        "if best_params['optimizer'] == \"SGD\":\n",
        "    final_optimizer = torch.optim.SGD(trainable_params, lr=best_params['lr'],\n",
        "                                       momentum=best_params.get('momentum', 0.9),\n",
        "                                       weight_decay=best_params.get('weight_decay', 1e-4))\n",
        "elif best_params['optimizer'] == \"AdamW\":\n",
        "    final_optimizer = torch.optim.AdamW(trainable_params, lr=best_params['lr'],\n",
        "                                         weight_decay=best_params.get('weight_decay', 1e-4))\n",
        "else:\n",
        "    final_optimizer = torch.optim.Adam(trainable_params, lr=best_params['lr'],\n",
        "                                        weight_decay=best_params.get('weight_decay', 1e-4))\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(final_optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "# Initialize W&B\n",
        "run = wandb.init(project=\"VanGogh_Classifier\", name=\"final_model_training\",\n",
        "                 config={**best_params, \"model\": \"VGG19\", \"training_type\": \"final\"})\n",
        "\n",
        "# Training loop with early stopping\n",
        "num_epochs = 25\n",
        "best_val_acc = 0.0\n",
        "best_model_state = None\n",
        "patience = 5\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "train_losses, val_losses, val_accuracies = [], [], []\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(final_model, final_train_loader, final_optimizer, criterion, device)\n",
        "    val_loss, val_acc = eval_one_epoch(final_model, final_val_loader, criterion, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    scheduler.step(val_acc)\n",
        "    wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss,\n",
        "               \"val_acc\": val_acc, \"lr\": final_optimizer.param_groups[0]['lr']})\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_state = {k: v.cpu().clone() for k, v in final_model.state_dict().items()}\n",
        "        epochs_without_improvement = 0\n",
        "        print(f\"  ‚úì New best model! (Val Acc: {best_val_acc:.4f})\")\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(f\"\\nEarly stopping after {epoch+1} epochs\")\n",
        "        break\n",
        "\n",
        "final_model.load_state_dict(best_model_state)\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training complete! Best validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "torch.save({'model_state_dict': best_model_state, 'best_params': best_params,\n",
        "            'best_val_acc': best_val_acc}, 'best_vangogh_classifier.pth')\n",
        "print(\"Model saved to 'best_vangogh_classifier.pth'\")\n",
        "\n",
        "run.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-ko9yReP-BW"
      },
      "source": [
        "## 8. Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErWsvIAaP-BW"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].plot(train_losses, label='Train Loss', color='#1f77b4', linewidth=2)\n",
        "axes[0].plot(val_losses, label='Validation Loss', color='#ff7f0e', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Validation Loss', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(val_accuracies, label='Validation Accuracy', color='#2ca02c', linewidth=2, marker='o')\n",
        "axes[1].axhline(y=best_val_acc, color='r', linestyle='--', label=f'Best: {best_val_acc:.4f}')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Validation Accuracy', fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png', dpi=150)\n",
        "plt.show()\n",
        "print(\"Saved: training_curves.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZs0WVCpP-BX"
      },
      "source": [
        "## 9. Test Set Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8M_t2gMP-BX"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False, num_workers=2)\n",
        "\n",
        "final_model.eval()\n",
        "all_preds, all_labels, all_probs = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = final_model(images)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_probs.extend(probs[:, 1].cpu().numpy())\n",
        "\n",
        "all_preds, all_labels, all_probs = np.array(all_preds), np.array(all_labels), np.array(all_probs)\n",
        "\n",
        "# Calculate metrics\n",
        "test_accuracy = accuracy_score(all_labels, all_preds)\n",
        "test_precision = precision_score(all_labels, all_preds)\n",
        "test_recall = recall_score(all_labels, all_preds)\n",
        "test_f1 = f1_score(all_labels, all_preds)\n",
        "test_auc = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TEST SET RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüìä Metrics:\")\n",
        "print(f\"   Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"   Precision: {test_precision:.4f}\")\n",
        "print(f\"   Recall:    {test_recall:.4f}\")\n",
        "print(f\"   F1-Score:  {test_f1:.4f}\")\n",
        "print(f\"   AUC-ROC:   {test_auc:.4f}\")\n",
        "\n",
        "print(f\"\\nüìã Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=['Not Van Gogh', 'Van Gogh']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-_b18W6P-BX"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix and ROC Curve\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['Not Van Gogh', 'Van Gogh'],\n",
        "            yticklabels=['Not Van Gogh', 'Van Gogh'], annot_kws={'size': 14})\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "axes[0].set_title('Confusion Matrix', fontweight='bold')\n",
        "\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "axes[1].plot(fpr, tpr, color='#1f77b4', linewidth=2, label=f'ROC (AUC = {test_auc:.4f})')\n",
        "axes[1].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "axes[1].fill_between(fpr, tpr, alpha=0.2, color='#1f77b4')\n",
        "axes[1].set_xlabel('False Positive Rate')\n",
        "axes[1].set_ylabel('True Positive Rate')\n",
        "axes[1].set_title('ROC Curve', fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix_roc.png', dpi=150)\n",
        "plt.show()\n",
        "print(\"Saved: confusion_matrix_roc.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeXTrTaHP-Bf"
      },
      "source": [
        "## 10. Final Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H9E0j2oP-Bi"
      },
      "outputs": [],
      "source": [
        "# Final Summary\n",
        "print(\"=\"*60)\n",
        "print(\"PART A - VAN GOGH CLASSIFIER - SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüìÅ Dataset:\")\n",
        "print(f\"   Total: {len(df)} | Van Gogh: {df['is_van_gogh'].sum()} | Other: {len(df)-df['is_van_gogh'].sum()}\")\n",
        "print(f\"   Split: 70% train / 15% val / 15% test\")\n",
        "\n",
        "print(\"\\nüîß Best Hyperparameters:\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"   {k}: {v}\")\n",
        "\n",
        "print(\"\\nüìä Performance:\")\n",
        "print(f\"   Best Val Acc:  {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
        "print(f\"   Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"   Test F1-Score: {test_f1:.4f}\")\n",
        "print(f\"   Test AUC-ROC:  {test_auc:.4f}\")\n",
        "\n",
        "print(\"\\nüíæ Saved Files:\")\n",
        "print(\"   best_vangogh_classifier.pth\")\n",
        "print(\"   training_curves.png\")\n",
        "print(\"   confusion_matrix_roc.png\")\n",
        "\n",
        "print(\"\\n‚úÖ Part A Complete!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}