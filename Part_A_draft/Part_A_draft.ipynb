{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h8XmJVfP-A8"
      },
      "source": [
        "# Part A - Van Gogh Painting Classifier\n",
        "\n",
        "## Deep Learning Project - Tel Aviv University\n",
        "\n",
        "This notebook implements a binary classifier to identify Van Gogh paintings using transfer learning with VGG19.\n",
        "\n",
        "### Overview:\n",
        "1. **Load Data**: Read pre-prepared CSV file with image paths and labels\n",
        "2. **Split Dataset**: 70% train, 15% validation, 15% test\n",
        "3. **Data Augmentation**: Apply transforms for training robustness\n",
        "4. **Model**: VGG19 pre-trained on ImageNet, fine-tuned for binary classification\n",
        "5. **Hyperparameter Tuning**: Use Optuna to find best parameters\n",
        "6. **Training**: Train final model with best hyperparameters\n",
        "7. **Evaluation**: Test set metrics and visualizations\n",
        "\n",
        "### Requirements:\n",
        "- Google Colab with GPU runtime\n",
        "- `post_impressionism_data.csv` file (created by Get_Post_Impressionism_Data.ipynb)\n",
        "- Weights & Biases account for experiment tracking\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iQGUIlOP-A-"
      },
      "source": [
        "## 1. Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQqOcR7cP-A_",
        "outputId": "9d920786-7eeb-469c-8298-7ab7119792f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running locally\n"
          ]
        }
      ],
      "source": [
        "# Check if running on Google Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running on Google Colab\")\n",
        "else:\n",
        "    print(\"Running locally\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T-wFi03lP-BA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
            "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages: optuna (hyperparameter tuning), wandb (experiment tracking)\n",
        "%pip install -q optuna wandb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj0h6Pw0P-BC"
      },
      "source": [
        "## 2. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8aoyWL1P-BF"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import platform\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, accuracy_score,\n",
        "    precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        ")\n",
        "\n",
        "# Hyperparameter tuning and logging\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# Hardware Benchmark Setup (Required by instructions)\n",
        "print(\"=\"*60)\n",
        "print(\"HARDWARE BENCHMARK INFORMATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Python version: {platform.python_version()}\")\n",
        "print(f\"Machine name (node): {platform.node()}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory Total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NTjxVeHP-BO"
      },
      "outputs": [],
      "source": [
        "# Setup device (GPU/CPU) - GPU is much faster for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device.type == 'cpu':\n",
        "    print(\"WARNING: Running on CPU. Enable GPU in Colab: Runtime -> Change runtime type -> GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CPU vs GPU Benchmark (Required by instruction 1.34)\n",
        "# Measure time for 20 forward+backward iterations on both CPU and GPU\n",
        "import time\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CPU vs GPU BENCHMARK\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Machine name: {platform.node()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create a dummy model for benchmarking\n",
        "dummy_model = nn.Sequential(\n",
        "    nn.Conv2d(3, 64, 3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.AdaptiveAvgPool2d(1),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(64, 2)\n",
        ")\n",
        "dummy_input = torch.randn(8, 3, 224, 224)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "dummy_labels = torch.randint(0, 2, (8,))\n",
        "\n",
        "# CPU Benchmark\n",
        "print(\"\\nRunning CPU benchmark (20 iterations)...\")\n",
        "cpu_model = dummy_model.to('cpu')\n",
        "cpu_input = dummy_input.to('cpu')\n",
        "cpu_labels = dummy_labels.to('cpu')\n",
        "cpu_optimizer = torch.optim.Adam(cpu_model.parameters(), lr=0.001)\n",
        "\n",
        "start_time = time.time()\n",
        "for _ in range(20):\n",
        "    cpu_optimizer.zero_grad()\n",
        "    outputs = cpu_model(cpu_input)\n",
        "    loss = criterion(outputs, cpu_labels)\n",
        "    loss.backward()\n",
        "    cpu_optimizer.step()\n",
        "cpu_time = time.time() - start_time\n",
        "\n",
        "print(f\"CPU time: {cpu_time:.4f} seconds ({cpu_time/20*1000:.2f} ms per iteration)\")\n",
        "\n",
        "# GPU Benchmark (if available)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"\\nRunning GPU benchmark (20 iterations)...\")\n",
        "    gpu_model = dummy_model.to('cuda')\n",
        "    gpu_input = dummy_input.to('cuda')\n",
        "    gpu_labels = dummy_labels.to('cuda')\n",
        "    gpu_optimizer = torch.optim.Adam(gpu_model.parameters(), lr=0.001)\n",
        "    \n",
        "    # Warmup\n",
        "    for _ in range(5):\n",
        "        gpu_optimizer.zero_grad()\n",
        "        outputs = gpu_model(gpu_input)\n",
        "        loss = criterion(outputs, gpu_labels)\n",
        "        loss.backward()\n",
        "        gpu_optimizer.step()\n",
        "    torch.cuda.synchronize()\n",
        "    \n",
        "    start_time = time.time()\n",
        "    for _ in range(20):\n",
        "        gpu_optimizer.zero_grad()\n",
        "        outputs = gpu_model(gpu_input)\n",
        "        loss = criterion(outputs, gpu_labels)\n",
        "        loss.backward()\n",
        "        gpu_optimizer.step()\n",
        "    torch.cuda.synchronize()\n",
        "    gpu_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"GPU time: {gpu_time:.4f} seconds ({gpu_time/20*1000:.2f} ms per iteration)\")\n",
        "    print(f\"\\nSpeedup: {cpu_time/gpu_time:.2f}x faster on GPU\")\n",
        "else:\n",
        "    print(\"\\nGPU not available, skipping GPU benchmark\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96eF9IOzP-BP"
      },
      "outputs": [],
      "source": [
        "# Login to Weights & Biases for experiment tracking\n",
        "wandb.login(key=\"16d1bc863b28f81253ac0ee253b453393791a7e1\")\n",
        "print(\"Logged in to Weights & Biases\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PylwidTjP-BP"
      },
      "source": [
        "## 3. Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scan Post_Impressionism directory and create metadata CSV\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Find Post_Impressionism directory (works in both Kaggle and Colab)\n",
        "possible_dirs = [\n",
        "    \"/kaggle/input/wikiart/Post_Impressionism\",  # Kaggle\n",
        "    \"/content/data/Post_Impressionism\",          # Colab\n",
        "    \"/content/Post_Impressionism\",               # Colab alternative\n",
        "    \"/content/wikiart/Post_Impressionism\",       # Colab alternative\n",
        "]\n",
        "\n",
        "base_dir = None\n",
        "for dir_path in possible_dirs:\n",
        "    if os.path.exists(dir_path):\n",
        "        base_dir = dir_path\n",
        "        break\n",
        "\n",
        "if base_dir is None:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Post_Impressionism directory not found!\\n\"\n",
        "        f\"   Checked: {possible_dirs}\\n\"\n",
        "        f\"   Please download images to one of these locations.\"\n",
        "    )\n",
        "\n",
        "print(f\"Found images in: {base_dir}\")\n",
        "\n",
        "# Scan directory and create DataFrame (like Nir)\n",
        "records = []\n",
        "for fname in os.listdir(base_dir):\n",
        "    if not fname.lower().endswith((\".jpg\", \".png\")):\n",
        "        continue\n",
        "    \n",
        "    artist = fname.split(\"_\")[0]  # Extract artist name from filename\n",
        "    \n",
        "    records.append({\n",
        "        \"filepath\": os.path.join(base_dir, fname),\n",
        "        \"filename\": fname,\n",
        "        \"artist\": artist,\n",
        "        \"is_van_gogh\": 1 if \"van-gogh\" in artist.lower() else 0\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "# Save metadata to CSV\n",
        "csv_output_path = \"/kaggle/working/post_impressionism_data.csv\" if os.path.exists(\"/kaggle\") else (\"/content/post_impressionism_data.csv\" if IN_COLAB else \"post_impressionism_data.csv\")\n",
        "df.to_csv(csv_output_path, index=False)\n",
        "print(f\"Saved metadata CSV: {csv_output_path}\")\n",
        "\n",
        "print(f\"\\nLoaded: {len(df)} images\")\n",
        "print(f\"  Van Gogh: {df['is_van_gogh'].sum()}\")\n",
        "print(f\"  Other: {len(df) - df['is_van_gogh'].sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLShT63aP-BR"
      },
      "outputs": [],
      "source": [
        "# Split dataset: 70% train, 15% validation, 15% test (stratified to maintain class balance)\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, test_size=0.3, stratify=df[\"is_van_gogh\"], random_state=SEED\n",
        ")\n",
        "\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.5, stratify=temp_df[\"is_van_gogh\"], random_state=SEED\n",
        ")\n",
        "\n",
        "print(\"Dataset splits:\")\n",
        "print(f\"  Train: {len(train_df):5d} ({len(train_df)/len(df):.1%})\")\n",
        "print(f\"  Val:   {len(val_df):5d} ({len(val_df)/len(df):.1%})\")\n",
        "print(f\"  Test:  {len(test_df):5d} ({len(test_df)/len(df):.1%})\")\n",
        "\n",
        "print(\"\\nClass distribution:\")\n",
        "print(f\"  Train - Van Gogh: {train_df['is_van_gogh'].mean():.2%}\")\n",
        "print(f\"  Val   - Van Gogh: {val_df['is_van_gogh'].mean():.2%}\")\n",
        "print(f\"  Test  - Van Gogh: {test_df['is_van_gogh'].mean():.2%}\")\n",
        "\n",
        "# Save dataset splits for later use (so data preparation doesn't need to be rerun)\n",
        "import pickle\n",
        "import os\n",
        "splits_save_path = 'dataset_splits.pkl'\n",
        "with open(splits_save_path, 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'train_df': train_df,\n",
        "        'val_df': val_df,\n",
        "        'test_df': test_df\n",
        "    }, f)\n",
        "print(f\"\\nDataset splits saved to '{splits_save_path}' (can skip data preparation next time)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwqo_KbRP-BS"
      },
      "source": [
        "## 4. Data Transforms & Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSzBqFT7P-BS"
      },
      "outputs": [],
      "source": [
        "# Define image transforms: resize to 224x224, normalize with ImageNet stats\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Training: data augmentation (random crop, flip, rotation, color jitter)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "])\n",
        "\n",
        "# Evaluation: no augmentation (consistent evaluation)\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "])\n",
        "\n",
        "print(\"Transforms defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXFfgteAP-BS"
      },
      "outputs": [],
      "source": [
        "# Custom PyTorch Dataset class to load images from file paths\n",
        "class VanGoghDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        # Filter out files that don't exist\n",
        "        self.df = self.df[self.df['filepath'].apply(os.path.exists)].reset_index(drop=True)\n",
        "        if len(self.df) < len(df):\n",
        "            print(f\"Warning: {len(df) - len(self.df)} files not found and removed\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        try:\n",
        "            image = Image.open(row[\"filepath\"]).convert(\"RGB\")\n",
        "            label = row[\"is_van_gogh\"]\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, label\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {row['filepath']}: {e}\")\n",
        "            # Return a black image as fallback (shouldn't happen if we filtered)\n",
        "            image = Image.new('RGB', (224, 224), color='black')\n",
        "            label = row[\"is_van_gogh\"]\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, label\n",
        "\n",
        "# Load dataset splits if they don't exist (in case data preparation wasn't run)\n",
        "if 'train_df' not in globals() or 'val_df' not in globals() or 'test_df' not in globals():\n",
        "    import pickle\n",
        "    import os\n",
        "    splits_save_path = 'dataset_splits.pkl'\n",
        "    if os.path.exists(splits_save_path):\n",
        "        print(f\"Loading dataset splits from {splits_save_path}...\")\n",
        "        with open(splits_save_path, 'rb') as f:\n",
        "            splits_data = pickle.load(f)\n",
        "        train_df = splits_data['train_df']\n",
        "        val_df = splits_data['val_df']\n",
        "        test_df = splits_data['test_df']\n",
        "        print(f\"Loaded dataset splits: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
        "    else:\n",
        "        raise NameError(\n",
        "            f\"Dataset splits not found and '{splits_save_path}' doesn't exist!\\n\"\n",
        "            \"   Please run data preparation steps (Steps 3-4) first.\"\n",
        "        )\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = VanGoghDataset(train_df, transform=train_transform)\n",
        "val_dataset = VanGoghDataset(val_df, transform=eval_transform)\n",
        "test_dataset = VanGoghDataset(test_df, transform=eval_transform)\n",
        "\n",
        "print(f\"Datasets: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bccCmJABP-BT"
      },
      "source": [
        "## 5. Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk0j02iOP-BT"
      },
      "outputs": [],
      "source": [
        "# Training and evaluation functions\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device, max_grad_norm=1.0):\n",
        "    \"\"\"Train for one epoch: forward pass, compute loss, backward pass, update weights\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        # Gradient clipping for training stability\n",
        "        if max_grad_norm > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if wandb.run is not None:\n",
        "            wandb.log({\"batch_loss\": loss.item()})\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(model, loader, criterion, device):\n",
        "    \"\"\"Evaluate: forward pass only, compute loss and accuracy\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return total_loss / len(loader), correct / total\n",
        "\n",
        "print(\"Training functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeGOP6L1P-BT"
      },
      "source": [
        "## 6. Hyperparameter Tuning with Optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoopVVlgP-BT"
      },
      "outputs": [],
      "source": [
        "# Create model: supports both VGG19 and AlexNet (project requirement)\n",
        "def create_model(model_name='VGG19', freeze_features=True, dropout=0.5):\n",
        "    \"\"\"\n",
        "    Create model with binary classifier.\n",
        "    Smart Fine-tuning Strategy:\n",
        "    - VGG19: Unfreeze last 8 layers (top blocks) when freeze_features=False\n",
        "    - AlexNet: Unfreeze last 2 layers (top layers) when freeze_features=False\n",
        "    \"\"\"\n",
        "    if model_name == 'VGG19':\n",
        "        model = models.vgg19(weights='IMAGENET1K_V1')\n",
        "        layers_to_unfreeze_count = 8  # Unfreeze roughly half the model (top blocks)\n",
        "    elif model_name == 'AlexNet':\n",
        "        model = models.alexnet(weights='IMAGENET1K_V1')\n",
        "        layers_to_unfreeze_count = 2  # Unfreeze only the very top layers\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "    \n",
        "    # 1. Freeze everything by default\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = False\n",
        "        \n",
        "    # 2. Smart Unfreeze (if requested)\n",
        "    if not freeze_features:\n",
        "        # Find all layers that have weights (trainable layers)\n",
        "        trainable_layers = [layer for layer in model.features if hasattr(layer, 'weight')]\n",
        "        \n",
        "        # Unfreeze only the last X layers based on model type\n",
        "        layers_to_unfreeze = trainable_layers[-layers_to_unfreeze_count:]\n",
        "        \n",
        "        for layer in layers_to_unfreeze:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = True\n",
        "                \n",
        "    # 3. Replace Classifier Head (Always trainable)\n",
        "    if model_name == 'VGG19':\n",
        "        num_features = model.classifier[6].in_features\n",
        "        model.classifier[6] = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(num_features, 2)\n",
        "        )\n",
        "    elif model_name == 'AlexNet':\n",
        "        num_features = model.classifier[6].in_features\n",
        "        model.classifier[6] = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(num_features, 2)\n",
        "        )\n",
        "    \n",
        "    return model.to(device)\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"Optuna objective: try hyperparameters, train model, return validation accuracy\"\"\"\n",
        "    # Optuna suggests hyperparameters\n",
        "    model_name = trial.suggest_categorical(\"model_name\", [\"VGG19\", \"AlexNet\"])  # Project requirement: both models\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-4, log=True)  # Reduced max LR for stability (was 1e-2)\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"AdamW\"])\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64])  # Added 8 for smaller GPUs\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
        "    momentum = trial.suggest_float(\"momentum\", 0.8, 0.99) if optimizer_name == \"SGD\" else 0.0\n",
        "    freeze_features = trial.suggest_categorical(\"freeze_features\", [True, False])\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.7)  # Added dropout search\n",
        "\n",
        "    # Create DataLoaders, model, optimizer\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)  # Like Nir\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    run = wandb.init(\n",
        "        project=\"VanGogh_Classifier\",\n",
        "        name=f\"trial_{trial.number}_{model_name}\",\n",
        "        config={\"model_name\": model_name, \"lr\": lr, \"optimizer\": optimizer_name, \"batch_size\": batch_size,\n",
        "                \"weight_decay\": weight_decay, \"momentum\": momentum,\n",
        "                \"freeze_features\": freeze_features, \"dropout\": dropout},\n",
        "        reinit=True\n",
        "    )\n",
        "    \n",
        "    # Define epoch as step metric to ensure X-axis starts at 0 for each trial\n",
        "    run.define_metric(\"epoch\", hidden=True)\n",
        "    run.define_metric(\"train_loss\", step_metric=\"epoch\")\n",
        "    run.define_metric(\"val_loss\", step_metric=\"epoch\")\n",
        "    run.define_metric(\"val_acc\", step_metric=\"epoch\")\n",
        "    run.define_metric(\"best_val_acc\", step_metric=\"epoch\")\n",
        "\n",
        "    model = create_model(model_name=model_name, freeze_features=freeze_features, dropout=dropout)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "\n",
        "    if optimizer_name == \"SGD\":\n",
        "        optimizer = torch.optim.SGD(trainable_params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    elif optimizer_name == \"AdamW\":\n",
        "        optimizer = torch.optim.AdamW(trainable_params, lr=lr, weight_decay=weight_decay)\n",
        "    else:\n",
        "        optimizer = torch.optim.Adam(trainable_params, lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # Train for a few epochs (quick evaluation for hyperparameter search)\n",
        "    num_epochs = 3  # Reduced for faster trials (30-60 min total search time)\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        val_loss, val_acc = eval_one_epoch(model, val_loader, criterion, device)\n",
        "        best_val_acc = max(best_val_acc, val_acc)\n",
        "        wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss,\n",
        "                   \"val_acc\": val_acc, \"best_val_acc\": best_val_acc}, step=epoch)\n",
        "        trial.report(val_acc, epoch)\n",
        "        if trial.should_prune():  # Stop bad trials early\n",
        "            run.finish()\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "    run.finish()\n",
        "    return best_val_acc\n",
        "\n",
        "print(\"Optuna objective function defined (supports VGG19 and AlexNet)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T2jcKusP-BU"
      },
      "outputs": [],
      "source": [
        "# Run hyperparameter search with Optuna\n",
        "# Project requirement: must take at least 30 minutes, max 60 minutes\n",
        "import optuna  # Ensure optuna is imported (in case cells run out of order)\n",
        "import time\n",
        "\n",
        "print(\"Starting hyperparameter search...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=2)\n",
        ")\n",
        "\n",
        "# Track start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Run optimization (max 60 minutes - will stop at timeout)\n",
        "study.optimize(objective, n_trials=10, timeout=3600, show_progress_bar=True)  # 10 trials for 30-60 min window\n",
        "\n",
        "# Calculate elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "elapsed_minutes = elapsed_time / 60\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HYPERPARAMETER SEARCH COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTime taken: {elapsed_minutes:.2f} minutes ({elapsed_time:.0f} seconds)\")\n",
        "print(f\"Completed trials: {len(study.trials)} / 10\")\n",
        "\n",
        "# Check project requirements\n",
        "if elapsed_minutes < 30:\n",
        "    print(f\"\\nWARNING: Search took only {elapsed_minutes:.2f} minutes!\")\n",
        "    print(\"   Project requires at least 30 minutes. Consider increasing n_trials.\")\n",
        "elif elapsed_minutes > 60:\n",
        "    print(f\"\\nWARNING: Search took {elapsed_minutes:.2f} minutes (exceeded 60 min limit)\")\n",
        "else:\n",
        "    print(f\"\\nTime requirement met: {elapsed_minutes:.2f} minutes (30-60 min range)\")\n",
        "\n",
        "print(f\"\\nBest validation accuracy: {study.best_value:.4f}\")\n",
        "print(f\"\\nBest hyperparameters:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Save study results for later use (so step 7 can run independently)\n",
        "import pickle\n",
        "study_save_path = 'optuna_study_results.pkl'\n",
        "with open(study_save_path, 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'best_params': study.best_params,\n",
        "        'best_value': study.best_value,\n",
        "        'n_trials': len(study.trials)\n",
        "    }, f)\n",
        "print(f\"\\nStudy results saved to '{study_save_path}' (can skip step 6 next time)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM7K6ZYQP-BV"
      },
      "source": [
        "## 7. Train Final Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU Utilization Monitoring (Required for appendix)\n",
        "# Run nvidia-smi to show GPU utilization and memory usage\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"GPU UTILIZATION (nvidia-smi)\")\n",
        "print(\"=\"*60)\n",
        "try:\n",
        "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=5)\n",
        "    print(result.stdout)\n",
        "    if result.stderr:\n",
        "        print(\"Stderr:\", result.stderr, file=sys.stderr)\n",
        "except FileNotFoundError:\n",
        "    print(\"nvidia-smi not found. This is expected if running on CPU or in some environments.\")\n",
        "    print(\"   GPU monitoring will be skipped.\")\n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"nvidia-smi timed out.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not run nvidia-smi: {e}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y6f0EkbP-BV"
      },
      "outputs": [],
      "source": [
        "# Train final model with best hyperparameters\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Load best_params if study doesn't exist (in case step 6 wasn't run)\n",
        "if 'study' not in globals() or study is None:\n",
        "    study_save_path = 'optuna_study_results.pkl'\n",
        "    if os.path.exists(study_save_path):\n",
        "        print(f\"Loading hyperparameters from {study_save_path}...\")\n",
        "        with open(study_save_path, 'rb') as f:\n",
        "            study_data = pickle.load(f)\n",
        "        best_params = study_data['best_params']\n",
        "        print(f\"Loaded hyperparameters (Best val acc: {study_data['best_value']:.4f})\")\n",
        "    else:\n",
        "        raise NameError(\n",
        "            f\"'study' not found and '{study_save_path}' doesn't exist!\\n\"\n",
        "            \"   Please run Step 6 (Hyperparameter Tuning) first.\"\n",
        "        )\n",
        "else:\n",
        "    best_params = study.best_params\n",
        "\n",
        "print(\"\\nTraining final model with best parameters:\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# Anti-overfitting improvements: increase dropout and weight decay\n",
        "# Optuna found dropout ~0.29, but we have overfitting - increase to 0.4 for better regularization\n",
        "original_dropout = best_params.get('dropout', 0.5)\n",
        "improved_dropout = max(0.4, min(0.5, original_dropout + 0.1))  # Increase by 0.1, cap at 0.5\n",
        "print(f\"\\nAnti-overfitting adjustments:\")\n",
        "print(f\"   Dropout: {original_dropout:.4f} -> {improved_dropout:.4f}\")\n",
        "\n",
        "original_weight_decay = best_params.get('weight_decay', 1e-4)\n",
        "improved_weight_decay = original_weight_decay * 1.25  # Increase by 25% for more regularization\n",
        "print(f\"   Weight decay: {original_weight_decay:.6f} â†’ {improved_weight_decay:.6f}\")\n",
        "\n",
        "# Setup: DataLoaders, model, optimizer, scheduler\n",
        "# Stability improvement: Manually set batch_size to 32 for final training (as per instructions)\n",
        "# This reduces 'zigzag' effect in loss curves, regardless of Optuna suggestion\n",
        "final_batch_size = 32  # Fixed batch size for stability (use 16 if memory is an issue)\n",
        "print(f\"\\nBatch size: Optuna suggested {best_params.get('batch_size', 'N/A')}, using {final_batch_size} for stability\")\n",
        "final_train_loader = DataLoader(train_dataset, batch_size=final_batch_size, shuffle=True, num_workers=2)\n",
        "final_val_loader = DataLoader(val_dataset, batch_size=final_batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "final_model = create_model(\n",
        "    model_name=best_params.get('model_name', 'VGG19'),\n",
        "    freeze_features=best_params.get('freeze_features', False),\n",
        "    dropout=improved_dropout  # Use improved dropout\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "trainable_params = filter(lambda p: p.requires_grad, final_model.parameters())\n",
        "\n",
        "if best_params['optimizer'] == \"SGD\":\n",
        "    final_optimizer = torch.optim.SGD(trainable_params, lr=best_params['lr'],\n",
        "                                       momentum=best_params.get('momentum', 0.9),\n",
        "                                       weight_decay=improved_weight_decay)  # Use improved weight decay\n",
        "elif best_params['optimizer'] == \"AdamW\":\n",
        "    final_optimizer = torch.optim.AdamW(trainable_params, lr=best_params['lr'],\n",
        "                                         weight_decay=improved_weight_decay)  # Use improved weight decay\n",
        "else:\n",
        "    final_optimizer = torch.optim.Adam(trainable_params, lr=best_params['lr'],\n",
        "                                        weight_decay=improved_weight_decay)  # Use improved weight decay\n",
        "\n",
        "# Improved learning rate scheduler: ReduceLROnPlateau with better settings\n",
        "# Use min_lr to prevent LR from going too low, and smaller factor for smoother decay\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    final_optimizer, mode='max', factor=0.5, patience=2, \n",
        "    min_lr=1e-6, threshold=0.001\n",
        ")\n",
        "\n",
        "# Training loop with early stopping\n",
        "run = wandb.init(project=\"VanGogh_Classifier\", name=\"final_model_training\",\n",
        "                 config={\n",
        "                     **best_params, \n",
        "                     \"training_type\": \"final\", \n",
        "                     \"gradient_clipping\": True, \n",
        "                     \"final_batch_size\": final_batch_size,\n",
        "                     \"freeze_features\": True,  # Log the actual parameter used\n",
        "                     \"improved_dropout\": improved_dropout,\n",
        "                     \"improved_weight_decay\": improved_weight_decay\n",
        "                 }, reinit=True)\n",
        "\n",
        "# Define epoch as step metric to ensure X-axis starts at 0 for final training\n",
        "run.define_metric(\"epoch\", hidden=True)\n",
        "run.define_metric(\"train_loss\", step_metric=\"epoch\")\n",
        "run.define_metric(\"val_loss\", step_metric=\"epoch\")\n",
        "run.define_metric(\"val_acc\", step_metric=\"epoch\")\n",
        "run.define_metric(\"lr\", step_metric=\"epoch\")\n",
        "\n",
        "num_epochs = 15  # Reduced for time efficiency\n",
        "best_val_acc = 0.0\n",
        "best_model_state = None\n",
        "patience = 5  # Increased early stopping patience (was 3) to allow more recovery\n",
        "epochs_without_improvement = 0\n",
        "train_losses, val_losses, val_accuracies = [], [], []\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "print(\"=\"*60)\n",
        "print(\"Improvements applied:\")\n",
        "print(\"   - Gradient clipping (max_norm=1.0) for stability\")\n",
        "print(\"   - Increased early stopping patience (3->5 epochs)\")\n",
        "print(\"   - Improved LR scheduler (min_lr, threshold)\")\n",
        "print(\"   - Increased dropout for better regularization\")\n",
        "print(\"   - Increased weight decay for more L2 regularization\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(final_model, final_train_loader, final_optimizer, criterion, device, max_grad_norm=1.0)\n",
        "    val_loss, val_acc = eval_one_epoch(final_model, final_val_loader, criterion, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    scheduler.step(val_acc)\n",
        "    current_lr = final_optimizer.param_groups[0]['lr']\n",
        "    \n",
        "    # Log to wandb with all parameters (graphs will be displayed in wandb)\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch, \n",
        "        \"train_loss\": train_loss, \n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_acc\": val_acc, \n",
        "        \"lr\": current_lr,\n",
        "        \"freeze_features\": True,  # Log the parameter\n",
        "        \"dropout\": improved_dropout,\n",
        "        \"weight_decay\": improved_weight_decay\n",
        "    }, step=epoch)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | LR: {current_lr:.2e}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_state = {k: v.cpu().clone() for k, v in final_model.state_dict().items()}\n",
        "        epochs_without_improvement = 0\n",
        "        print(f\"  New best model! (Val Acc: {best_val_acc:.4f})\")\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(f\"\\nEarly stopping after {epoch+1} epochs\")\n",
        "        break\n",
        "\n",
        "final_model.load_state_dict(best_model_state)\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training complete! Best validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "# Save model and training history for later use (so you don't need to retrain)\n",
        "torch.save({\n",
        "    'model_state_dict': best_model_state, \n",
        "    'best_params': best_params,\n",
        "    'best_val_acc': best_val_acc,\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses,\n",
        "    'val_accuracies': val_accuracies\n",
        "}, 'best_vangogh_classifier.pth')\n",
        "print(\"Model and training history saved to 'best_vangogh_classifier.pth'\")\n",
        "\n",
        "run.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-ko9yReP-BW"
      },
      "source": [
        "## 8. Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErWsvIAaP-BW"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "import matplotlib.pyplot as plt  # Ensure matplotlib is imported\n",
        "import os\n",
        "import torch  # Import torch for loading checkpoint\n",
        "\n",
        "# Load training history if variables don't exist (in case kernel was restarted)\n",
        "if 'train_losses' not in globals() or 'val_losses' not in globals() or 'val_accuracies' not in globals() or 'best_val_acc' not in globals():\n",
        "    checkpoint_path = 'best_vangogh_classifier.pth'\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading training history from {checkpoint_path}...\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        train_losses = checkpoint.get('train_losses', [])\n",
        "        val_losses = checkpoint.get('val_losses', [])\n",
        "        val_accuracies = checkpoint.get('val_accuracies', [])\n",
        "        best_val_acc = checkpoint.get('best_val_acc', 0.0)\n",
        "        print(f\"Loaded training history: {len(train_losses)} epochs, Best Val Acc: {best_val_acc:.4f}\")\n",
        "    else:\n",
        "        raise NameError(\n",
        "            f\"Training variables not found and checkpoint file '{checkpoint_path}' doesn't exist!\\n\"\n",
        "            \"   Please run Step 7 (Train Final Model) first before plotting.\"\n",
        "        )\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].plot(train_losses, label='Train Loss', color='#1f77b4', linewidth=2)\n",
        "axes[0].plot(val_losses, label='Validation Loss', color='#ff7f0e', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Validation Loss', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(val_accuracies, label='Validation Accuracy', color='#2ca02c', linewidth=2, marker='o')\n",
        "axes[1].axhline(y=best_val_acc, color='r', linestyle='--', label=f'Best: {best_val_acc:.4f}')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Validation Accuracy', fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png', dpi=150)\n",
        "plt.show()\n",
        "print(\"Saved: training_curves.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZs0WVCpP-BX"
      },
      "source": [
        "## 9. Test Set Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8M_t2gMP-BX"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test set (final performance metric)\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Load best_params if not available (needed for batch_size)\n",
        "if 'best_params' not in globals():\n",
        "    checkpoint_path = 'best_vangogh_classifier.pth'\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        best_params = checkpoint.get('best_params')\n",
        "        if best_params:\n",
        "            print(f\"Loaded best_params from checkpoint\")\n",
        "    if 'best_params' not in globals() or best_params is None:\n",
        "        import pickle\n",
        "        study_save_path = 'optuna_study_results.pkl'\n",
        "        if os.path.exists(study_save_path):\n",
        "            with open(study_save_path, 'rb') as f:\n",
        "                study_data = pickle.load(f)\n",
        "            best_params = study_data['best_params']\n",
        "            print(f\"Loaded best_params from study file\")\n",
        "        else:\n",
        "            raise NameError(\n",
        "                \"Cannot find best_params!\\n\"\n",
        "                \"   Please run Step 6 (Hyperparameter Tuning) or Step 7 (Train Final Model) first.\"\n",
        "            )\n",
        "\n",
        "# Check if final_model exists (step 7 must be run for evaluation)\n",
        "if 'final_model' not in globals():\n",
        "    raise NameError(\n",
        "        \"'final_model' not found!\\n\"\n",
        "        \"   Please run Step 7 (Train Final Model) first before evaluation.\\n\"\n",
        "        \"   The model needs to be created and trained in step 7.\"\n",
        "    )\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False, num_workers=2)\n",
        "\n",
        "final_model.eval()\n",
        "all_preds, all_labels, all_probs = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = final_model(images)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_probs.extend(probs[:, 1].cpu().numpy())\n",
        "\n",
        "all_preds, all_labels, all_probs = np.array(all_preds), np.array(all_labels), np.array(all_probs)\n",
        "\n",
        "# Calculate metrics\n",
        "test_accuracy = accuracy_score(all_labels, all_preds)\n",
        "test_precision = precision_score(all_labels, all_preds)\n",
        "test_recall = recall_score(all_labels, all_preds)\n",
        "test_f1 = f1_score(all_labels, all_preds)\n",
        "test_auc = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TEST SET RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nMetrics:\")\n",
        "print(f\"   Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"   Precision: {test_precision:.4f}\")\n",
        "print(f\"   Recall:    {test_recall:.4f}\")\n",
        "print(f\"   F1-Score:  {test_f1:.4f}\")\n",
        "print(f\"   AUC-ROC:   {test_auc:.4f}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=['Not Van Gogh', 'Van Gogh']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-_b18W6P-BX"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix and ROC Curve\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['Not Van Gogh', 'Van Gogh'],\n",
        "            yticklabels=['Not Van Gogh', 'Van Gogh'], annot_kws={'size': 14})\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "axes[0].set_title('Confusion Matrix', fontweight='bold')\n",
        "\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "axes[1].plot(fpr, tpr, color='#1f77b4', linewidth=2, label=f'ROC (AUC = {test_auc:.4f})')\n",
        "axes[1].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "axes[1].fill_between(fpr, tpr, alpha=0.2, color='#1f77b4')\n",
        "axes[1].set_xlabel('False Positive Rate')\n",
        "axes[1].set_ylabel('True Positive Rate')\n",
        "axes[1].set_title('ROC Curve', fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix_roc.png', dpi=150)\n",
        "plt.show()\n",
        "print(\"Saved: confusion_matrix_roc.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize False Positives (images predicted as Van Gogh but are not)\n",
        "# This is important for analysis in Part 3\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Find False Positives\n",
        "false_positives = []\n",
        "for idx, (pred, label, prob) in enumerate(zip(all_preds, all_labels, all_probs)):\n",
        "    if pred == 1 and label == 0:  # Predicted Van Gogh but is not\n",
        "        false_positives.append((idx, prob))\n",
        "\n",
        "# Sort by confidence (highest probability first)\n",
        "false_positives.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Display top False Positives\n",
        "num_to_show = min(12, len(false_positives))\n",
        "if num_to_show > 0:\n",
        "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
        "    fig.suptitle('False Positives: Predicted as Van Gogh (but are not)', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    for i, (idx, prob) in enumerate(false_positives[:num_to_show]):\n",
        "        row = i // 4\n",
        "        col = i % 4\n",
        "        \n",
        "        # Get image path from test dataset\n",
        "        image_path = test_df.iloc[idx]['filepath']\n",
        "        image = Image.open(image_path)\n",
        "        \n",
        "        axes[row, col].imshow(image)\n",
        "        axes[row, col].set_title(f'Conf: {prob:.3f}\\n{test_df.iloc[idx][\"artist\"]}', fontsize=10)\n",
        "        axes[row, col].axis('off')\n",
        "    \n",
        "    # Hide empty subplots\n",
        "    for i in range(num_to_show, 12):\n",
        "        row = i // 4\n",
        "        col = i % 4\n",
        "        axes[row, col].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('false_positives.png', dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"Saved: false_positives.png\")\n",
        "    print(f\"\\nTotal False Positives: {len(false_positives)} / {len(all_preds)}\")\n",
        "else:\n",
        "    print(\"No False Positives found!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeXTrTaHP-Bf"
      },
      "source": [
        "## 10. Final Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H9E0j2oP-Bi"
      },
      "outputs": [],
      "source": [
        "# Final Summary\n",
        "print(\"=\"*60)\n",
        "print(\"PART A - VAN GOGH CLASSIFIER - SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nDataset:\")\n",
        "print(f\"   Total: {len(df)} | Van Gogh: {df['is_van_gogh'].sum()} | Other: {len(df)-df['is_van_gogh'].sum()}\")\n",
        "print(f\"   Split: 70% train / 15% val / 15% test\")\n",
        "\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"   {k}: {v}\")\n",
        "\n",
        "print(\"\\nPerformance:\")\n",
        "print(f\"   Best Val Acc:  {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
        "print(f\"   Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"   Test F1-Score: {test_f1:.4f}\")\n",
        "print(f\"   Test AUC-ROC:  {test_auc:.4f}\")\n",
        "\n",
        "print(\"\\nSaved Files:\")\n",
        "print(\"   best_vangogh_classifier.pth\")\n",
        "print(\"   training_curves.png\")\n",
        "print(\"   confusion_matrix_roc.png\")\n",
        "\n",
        "print(\"\\nPart A Complete!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
