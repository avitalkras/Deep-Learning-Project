{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h8XmJVfP-A8"
      },
      "source": [
        "# Part A - Van Gogh Painting Classifier\n",
        "\n",
        "## Deep Learning Project - Tel Aviv University\n",
        "\n",
        "This notebook implements a binary classifier to identify Van Gogh paintings using transfer learning with VGG19.\n",
        "\n",
        "### Requirements:\n",
        "- Google Colab with GPU runtime\n",
        "- `post_impressionism_data.csv` file in the project (created by Get_Post_Impressionism_Data.ipynb)\n",
        "- Weights & Biases account for experiment tracking\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iQGUIlOP-A-"
      },
      "source": [
        "## 1. Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQqOcR7cP-A_",
        "outputId": "9d920786-7eeb-469c-8298-7ab7119792f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Running on Google Colab\n",
            "‚úì Connected via Git repository\n"
          ]
        }
      ],
      "source": [
        "# Check environment\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"‚úì Running on Google Colab\")\n",
        "else:\n",
        "    print(\"Running locally\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-wFi03lP-BA"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q optuna wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eps1cNyuP-BB",
        "outputId": "0591aa05-a57c-474d-89f8-06897b8ad926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Kaggle API configured successfully\n",
            "  Username: avitalkrasovitskii\n",
            "  Key: 53241c7887...\n"
          ]
        }
      ],
      "source": [
        "# Dataset is already prepared - using post_impressionism_data.csv\n",
        "# (Created by Get_Post_Impressionism_Data.ipynb)\n",
        "print(\"‚úì Using pre-prepared dataset from CSV file\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le-VZOv-P-BB",
        "outputId": "c32fe519-e359-4437-9f75-442f85264ef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/steubk/wikiart\n",
            "License(s): CC0-1.0\n"
          ]
        }
      ],
      "source": [
        "# Dataset preparation skipped - using CSV file instead\n",
        "# See Get_Post_Impressionism_Data.ipynb for dataset preparation\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset extraction skipped - using CSV file instead\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj0h6Pw0P-BC"
      },
      "source": [
        "## 2. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8aoyWL1P-BF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    roc_curve\n",
        ")\n",
        "\n",
        "# Hyperparameter tuning and logging\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NTjxVeHP-BO"
      },
      "outputs": [],
      "source": [
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device.type == 'cpu':\n",
        "    print(\"‚ö†Ô∏è WARNING: Running on CPU. For faster training, enable GPU in Colab:\")\n",
        "    print(\"   Runtime -> Change runtime type -> Hardware accelerator -> GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96eF9IOzP-BP"
      },
      "outputs": [],
      "source": [
        "# Login to Weights & Biases\n",
        "wandb.login(key=\"16d1bc863b28f81253ac0ee253b453393791a7e1\")\n",
        "print(\"‚úì Logged in to Weights & Biases\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PylwidTjP-BP"
      },
      "source": [
        "## 3. Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset from CSV file\n",
        "# The CSV file should be in the project directory or uploaded to Colab\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "csv_paths = [\n",
        "    \"post_impressionism_data.csv\",  # Current directory (if opened from GitHub)\n",
        "    \"/content/post_impressionism_data.csv\",  # Colab\n",
        "    \"/kaggle/working/post_impressionism_data.csv\",  # Kaggle\n",
        "]\n",
        "\n",
        "csv_path = None\n",
        "for path in csv_paths:\n",
        "    if os.path.exists(path):\n",
        "        csv_path = path\n",
        "        break\n",
        "\n",
        "if csv_path is None:\n",
        "    # Try to upload if in Colab\n",
        "    if IN_COLAB:\n",
        "        print(\"‚ö†Ô∏è CSV file not found in standard locations.\")\n",
        "        print(\"   Please upload post_impressionism_data.csv to Colab:\")\n",
        "        print(\"   Files -> Upload to session storage\")\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            uploaded = files.upload()\n",
        "            for filename in uploaded.keys():\n",
        "                if filename.endswith('.csv'):\n",
        "                    csv_path = filename\n",
        "                    break\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    if csv_path is None:\n",
        "        raise FileNotFoundError(\n",
        "            f\"‚ùå CSV file not found!\\n\"\n",
        "            f\"  Please upload post_impressionism_data.csv or run Get_Post_Impressionism_Data.ipynb first.\\n\"\n",
        "            f\"  Checked: {csv_paths}\"\n",
        "        )\n",
        "\n",
        "print(f\"‚úì Loading dataset from: {csv_path}\")\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "print(f\"\\n‚úì DataFrame loaded!\")\n",
        "print(f\"  Total: {len(df)} images\")\n",
        "print(f\"  Van Gogh: {df['is_van_gogh'].sum()}\")\n",
        "print(f\"  Other: {len(df) - df['is_van_gogh'].sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U05M5vt3P-BR"
      },
      "outputs": [],
      "source": [
        "# OLD CODE - Replaced by Cell 12 (CSV loading)\n",
        "# This cell kept for reference but not used\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLShT63aP-BR"
      },
      "outputs": [],
      "source": [
        "# Split dataset: 70% train, 15% validation, 15% test (stratified)\n",
        "\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, test_size=0.3, stratify=df[\"is_van_gogh\"], random_state=SEED\n",
        ")\n",
        "\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.5, stratify=temp_df[\"is_van_gogh\"], random_state=SEED\n",
        ")\n",
        "\n",
        "print(\"Dataset splits:\")\n",
        "print(f\"  Train: {len(train_df):5d} images ({len(train_df)/len(df):.1%})\")\n",
        "print(f\"  Val:   {len(val_df):5d} images ({len(val_df)/len(df):.1%})\")\n",
        "print(f\"  Test:  {len(test_df):5d} images ({len(test_df)/len(df):.1%})\")\n",
        "\n",
        "print(\"\\nClass distribution per split:\")\n",
        "print(f\"  Train - Van Gogh: {train_df['is_van_gogh'].mean():.2%}\")\n",
        "print(f\"  Val   - Van Gogh: {val_df['is_van_gogh'].mean():.2%}\")\n",
        "print(f\"  Test  - Van Gogh: {test_df['is_van_gogh'].mean():.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwqo_KbRP-BS"
      },
      "source": [
        "## 4. Data Transforms & Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSzBqFT7P-BS"
      },
      "outputs": [],
      "source": [
        "# Define transforms (ImageNet normalization for VGG19)\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Training transforms with data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "])\n",
        "\n",
        "# Evaluation transforms (no augmentation)\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "])\n",
        "\n",
        "print(\"Transforms defined ‚úì\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXFfgteAP-BS"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset class\n",
        "class VanGoghDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image = Image.open(row[\"filepath\"]).convert(\"RGB\")\n",
        "        label = row[\"is_van_gogh\"]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = VanGoghDataset(train_df, transform=train_transform)\n",
        "val_dataset = VanGoghDataset(val_df, transform=eval_transform)\n",
        "test_dataset = VanGoghDataset(test_df, transform=eval_transform)\n",
        "\n",
        "print(f\"Datasets created:\")\n",
        "print(f\"  Train: {len(train_dataset)} samples\")\n",
        "print(f\"  Val:   {len(val_dataset)} samples\")\n",
        "print(f\"  Test:  {len(test_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bccCmJABP-BT"
      },
      "source": [
        "## 5. Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk0j02iOP-BT"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    \"\"\"Train model for one epoch. Returns average loss.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if wandb.run is not None:\n",
        "            wandb.log({\"batch_loss\": loss.item()})\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(model, loader, criterion, device):\n",
        "    \"\"\"Evaluate model. Returns average loss and accuracy.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return total_loss / len(loader), correct / total\n",
        "\n",
        "print(\"Training functions defined ‚úì\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeGOP6L1P-BT"
      },
      "source": [
        "## 6. Hyperparameter Tuning with Optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoopVVlgP-BT"
      },
      "outputs": [],
      "source": [
        "def create_model(freeze_features=True):\n",
        "    \"\"\"Create VGG19 model with modified classifier for binary classification.\"\"\"\n",
        "    model = models.vgg19(weights='IMAGENET1K_V1')\n",
        "\n",
        "    if freeze_features:\n",
        "        for param in model.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, 2)\n",
        "    return model.to(device)\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"Optuna objective function for hyperparameter optimization.\"\"\"\n",
        "    # Suggest hyperparameters\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"AdamW\"])\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
        "    momentum = trial.suggest_float(\"momentum\", 0.8, 0.99) if optimizer_name == \"SGD\" else 0.0\n",
        "    freeze_features = trial.suggest_categorical(\"freeze_features\", [True, False])\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(\n",
        "        project=\"VanGogh_Classifier\",\n",
        "        name=f\"trial_{trial.number}\",\n",
        "        config={\"lr\": lr, \"optimizer\": optimizer_name, \"batch_size\": batch_size,\n",
        "                \"weight_decay\": weight_decay, \"momentum\": momentum,\n",
        "                \"freeze_features\": freeze_features, \"model\": \"VGG19\"},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # Create model and optimizer\n",
        "    model = create_model(freeze_features=freeze_features)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "\n",
        "    if optimizer_name == \"SGD\":\n",
        "        optimizer = torch.optim.SGD(trainable_params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    elif optimizer_name == \"AdamW\":\n",
        "        optimizer = torch.optim.AdamW(trainable_params, lr=lr, weight_decay=weight_decay)\n",
        "    else:\n",
        "        optimizer = torch.optim.Adam(trainable_params, lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 10\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        val_loss, val_acc = eval_one_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "        best_val_acc = max(best_val_acc, val_acc)\n",
        "\n",
        "        wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss,\n",
        "                   \"val_acc\": val_acc, \"best_val_acc\": best_val_acc})\n",
        "\n",
        "        trial.report(val_acc, epoch)\n",
        "        if trial.should_prune():\n",
        "            run.finish()\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    run.finish()\n",
        "    return best_val_acc\n",
        "\n",
        "print(\"Optuna objective function defined ‚úì\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T2jcKusP-BU"
      },
      "outputs": [],
      "source": [
        "# Run hyperparameter search\n",
        "print(\"Starting hyperparameter search...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=2)\n",
        ")\n",
        "\n",
        "# Adjust n_trials and timeout based on your time constraints\n",
        "study.optimize(objective, n_trials=15, timeout=3600, show_progress_bar=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HYPERPARAMETER SEARCH COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nBest validation accuracy: {study.best_value:.4f}\")\n",
        "print(f\"\\nBest hyperparameters:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VzTqTUXP-BU"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM7K6ZYQP-BV"
      },
      "source": [
        "## 7. Train Final Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y6f0EkbP-BV"
      },
      "outputs": [],
      "source": [
        "# Get best parameters and train final model\n",
        "best_params = study.best_params\n",
        "print(\"Training final model with best parameters:\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# Create DataLoaders\n",
        "final_train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True, num_workers=2)\n",
        "final_val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False, num_workers=2)\n",
        "\n",
        "# Create model\n",
        "final_model = create_model(freeze_features=best_params.get('freeze_features', False))\n",
        "\n",
        "# Setup optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "trainable_params = filter(lambda p: p.requires_grad, final_model.parameters())\n",
        "\n",
        "if best_params['optimizer'] == \"SGD\":\n",
        "    final_optimizer = torch.optim.SGD(trainable_params, lr=best_params['lr'],\n",
        "                                       momentum=best_params.get('momentum', 0.9),\n",
        "                                       weight_decay=best_params.get('weight_decay', 1e-4))\n",
        "elif best_params['optimizer'] == \"AdamW\":\n",
        "    final_optimizer = torch.optim.AdamW(trainable_params, lr=best_params['lr'],\n",
        "                                         weight_decay=best_params.get('weight_decay', 1e-4))\n",
        "else:\n",
        "    final_optimizer = torch.optim.Adam(trainable_params, lr=best_params['lr'],\n",
        "                                        weight_decay=best_params.get('weight_decay', 1e-4))\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(final_optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "# Initialize W&B\n",
        "run = wandb.init(project=\"VanGogh_Classifier\", name=\"final_model_training\",\n",
        "                 config={**best_params, \"model\": \"VGG19\", \"training_type\": \"final\"})\n",
        "\n",
        "# Training loop with early stopping\n",
        "num_epochs = 25\n",
        "best_val_acc = 0.0\n",
        "best_model_state = None\n",
        "patience = 5\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "train_losses, val_losses, val_accuracies = [], [], []\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(final_model, final_train_loader, final_optimizer, criterion, device)\n",
        "    val_loss, val_acc = eval_one_epoch(final_model, final_val_loader, criterion, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    scheduler.step(val_acc)\n",
        "    wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss,\n",
        "               \"val_acc\": val_acc, \"lr\": final_optimizer.param_groups[0]['lr']})\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_state = {k: v.cpu().clone() for k, v in final_model.state_dict().items()}\n",
        "        epochs_without_improvement = 0\n",
        "        print(f\"  ‚úì New best model! (Val Acc: {best_val_acc:.4f})\")\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(f\"\\nEarly stopping after {epoch+1} epochs\")\n",
        "        break\n",
        "\n",
        "final_model.load_state_dict(best_model_state)\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training complete! Best validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "torch.save({'model_state_dict': best_model_state, 'best_params': best_params,\n",
        "            'best_val_acc': best_val_acc}, 'best_vangogh_classifier.pth')\n",
        "print(\"Model saved to 'best_vangogh_classifier.pth'\")\n",
        "\n",
        "run.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-ko9yReP-BW"
      },
      "source": [
        "## 8. Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErWsvIAaP-BW"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].plot(train_losses, label='Train Loss', color='#1f77b4', linewidth=2)\n",
        "axes[0].plot(val_losses, label='Validation Loss', color='#ff7f0e', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Validation Loss', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(val_accuracies, label='Validation Accuracy', color='#2ca02c', linewidth=2, marker='o')\n",
        "axes[1].axhline(y=best_val_acc, color='r', linestyle='--', label=f'Best: {best_val_acc:.4f}')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Validation Accuracy', fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png', dpi=150)\n",
        "plt.show()\n",
        "print(\"Saved: training_curves.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZs0WVCpP-BX"
      },
      "source": [
        "## 9. Test Set Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8M_t2gMP-BX"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False, num_workers=2)\n",
        "\n",
        "final_model.eval()\n",
        "all_preds, all_labels, all_probs = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = final_model(images)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_probs.extend(probs[:, 1].cpu().numpy())\n",
        "\n",
        "all_preds, all_labels, all_probs = np.array(all_preds), np.array(all_labels), np.array(all_probs)\n",
        "\n",
        "# Calculate metrics\n",
        "test_accuracy = accuracy_score(all_labels, all_preds)\n",
        "test_precision = precision_score(all_labels, all_preds)\n",
        "test_recall = recall_score(all_labels, all_preds)\n",
        "test_f1 = f1_score(all_labels, all_preds)\n",
        "test_auc = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TEST SET RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüìä Metrics:\")\n",
        "print(f\"   Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"   Precision: {test_precision:.4f}\")\n",
        "print(f\"   Recall:    {test_recall:.4f}\")\n",
        "print(f\"   F1-Score:  {test_f1:.4f}\")\n",
        "print(f\"   AUC-ROC:   {test_auc:.4f}\")\n",
        "\n",
        "print(f\"\\nüìã Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=['Not Van Gogh', 'Van Gogh']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-_b18W6P-BX"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix and ROC Curve\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['Not Van Gogh', 'Van Gogh'],\n",
        "            yticklabels=['Not Van Gogh', 'Van Gogh'], annot_kws={'size': 14})\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "axes[0].set_title('Confusion Matrix', fontweight='bold')\n",
        "\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "axes[1].plot(fpr, tpr, color='#1f77b4', linewidth=2, label=f'ROC (AUC = {test_auc:.4f})')\n",
        "axes[1].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "axes[1].fill_between(fpr, tpr, alpha=0.2, color='#1f77b4')\n",
        "axes[1].set_xlabel('False Positive Rate')\n",
        "axes[1].set_ylabel('True Positive Rate')\n",
        "axes[1].set_title('ROC Curve', fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix_roc.png', dpi=150)\n",
        "plt.show()\n",
        "print(\"Saved: confusion_matrix_roc.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeXTrTaHP-Bf"
      },
      "source": [
        "## 10. Final Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H9E0j2oP-Bi"
      },
      "outputs": [],
      "source": [
        "# Final Summary\n",
        "print(\"=\"*60)\n",
        "print(\"PART A - VAN GOGH CLASSIFIER - SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüìÅ Dataset:\")\n",
        "print(f\"   Total: {len(df)} | Van Gogh: {df['is_van_gogh'].sum()} | Other: {len(df)-df['is_van_gogh'].sum()}\")\n",
        "print(f\"   Split: 70% train / 15% val / 15% test\")\n",
        "\n",
        "print(\"\\nüîß Best Hyperparameters:\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"   {k}: {v}\")\n",
        "\n",
        "print(\"\\nüìä Performance:\")\n",
        "print(f\"   Best Val Acc:  {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
        "print(f\"   Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"   Test F1-Score: {test_f1:.4f}\")\n",
        "print(f\"   Test AUC-ROC:  {test_auc:.4f}\")\n",
        "\n",
        "print(\"\\nüíæ Saved Files:\")\n",
        "print(\"   best_vangogh_classifier.pth\")\n",
        "print(\"   training_curves.png\")\n",
        "print(\"   confusion_matrix_roc.png\")\n",
        "\n",
        "print(\"\\n‚úÖ Part A Complete!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
