{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8h8XmJVfP-A8"
   },
   "source": [
    "# Part A - Van Gogh Painting Classifier\n",
    "\n",
    "## Deep Learning Project - Tel Aviv University\n",
    "\n",
    "This notebook implements a binary classifier to identify Van Gogh paintings using transfer learning with VGG19.\n",
    "\n",
    "### Overview:\n",
    "1. **Load Data**: Read pre-prepared CSV file with image paths and labels\n",
    "2. **Split Dataset**: 70% train, 15% validation, 15% test\n",
    "3. **Data Augmentation**: Apply transforms for training robustness\n",
    "4. **Model**: VGG19 pre-trained on ImageNet, fine-tuned for binary classification\n",
    "5. **Hyperparameter Tuning**: Use Optuna to find best parameters\n",
    "6. **Training**: Train final model with best hyperparameters\n",
    "7. **Evaluation**: Test set metrics and visualizations\n",
    "\n",
    "### Requirements:\n",
    "- Google Colab with GPU runtime\n",
    "- `post_impressionism_data.csv` file (created by Get_Post_Impressionism_Data.ipynb)\n",
    "- Weights & Biases account for experiment tracking\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iQGUIlOP-A-"
   },
   "source": [
    "## 1. Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQqOcR7cP-A_",
    "outputId": "9d920786-7eeb-469c-8298-7ab7119792f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n"
     ]
    }
   ],
   "source": [
    "# Check if running on Google Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"‚úì Running on Google Colab\")\n",
    "else:\n",
    "    print(\"Running locally\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "T-wFi03lP-BA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages: optuna (hyperparameter tuning), wandb (experiment tracking)\n",
    "%pip install -q optuna wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zj0h6Pw0P-BC"
   },
   "source": [
    "## 2. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8aoyWL1P-BF"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning and logging\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NTjxVeHP-BO"
   },
   "outputs": [],
   "source": [
    "# Setup device (GPU/CPU) - GPU is much faster for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cpu':\n",
    "    print(\"‚ö†Ô∏è WARNING: Running on CPU. Enable GPU in Colab: Runtime -> Change runtime type -> GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96eF9IOzP-BP"
   },
   "outputs": [],
   "source": [
    "# Login to Weights & Biases for experiment tracking\n",
    "wandb.login(key=\"16d1bc863b28f81253ac0ee253b453393791a7e1\")\n",
    "print(\"‚úì Logged in to Weights & Biases\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PylwidTjP-BP"
   },
   "source": [
    "## 3. Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan Post_Impressionism directory and create metadata CSV\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Find Post_Impressionism directory (works in both Kaggle and Colab)\n",
    "possible_dirs = [\n",
    "    \"/kaggle/input/wikiart/Post_Impressionism\",  # Kaggle\n",
    "    \"/content/data/Post_Impressionism\",          # Colab\n",
    "    \"/content/Post_Impressionism\",               # Colab alternative\n",
    "    \"/content/wikiart/Post_Impressionism\",       # Colab alternative\n",
    "]\n",
    "\n",
    "base_dir = None\n",
    "for dir_path in possible_dirs:\n",
    "    if os.path.exists(dir_path):\n",
    "        base_dir = dir_path\n",
    "        break\n",
    "\n",
    "if base_dir is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Post_Impressionism directory not found!\\n\"\n",
    "        f\"   Checked: {possible_dirs}\\n\"\n",
    "        f\"   Please download images to one of these locations.\"\n",
    "    )\n",
    "\n",
    "print(f\"‚úì Found images in: {base_dir}\")\n",
    "\n",
    "# Scan directory and create DataFrame (like Nir)\n",
    "records = []\n",
    "for fname in os.listdir(base_dir):\n",
    "    if not fname.lower().endswith((\".jpg\", \".png\")):\n",
    "        continue\n",
    "    \n",
    "    artist = fname.split(\"_\")[0]  # Extract artist name from filename\n",
    "    \n",
    "    records.append({\n",
    "        \"filepath\": os.path.join(base_dir, fname),\n",
    "        \"filename\": fname,\n",
    "        \"artist\": artist,\n",
    "        \"is_van_gogh\": 1 if \"van-gogh\" in artist.lower() else 0\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save metadata to CSV\n",
    "csv_output_path = \"/kaggle/working/post_impressionism_data.csv\" if os.path.exists(\"/kaggle\") else (\"/content/post_impressionism_data.csv\" if IN_COLAB else \"post_impressionism_data.csv\")\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "print(f\"‚úì Saved metadata CSV: {csv_output_path}\")\n",
    "\n",
    "print(f\"\\n‚úì Loaded: {len(df)} images\")\n",
    "print(f\"  Van Gogh: {df['is_van_gogh'].sum()}\")\n",
    "print(f\"  Other: {len(df) - df['is_van_gogh'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLShT63aP-BR"
   },
   "outputs": [],
   "source": [
    "# Split dataset: 70% train, 15% validation, 15% test (stratified to maintain class balance)\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.3, stratify=df[\"is_van_gogh\"], random_state=SEED\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df[\"is_van_gogh\"], random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"  Train: {len(train_df):5d} ({len(train_df)/len(df):.1%})\")\n",
    "print(f\"  Val:   {len(val_df):5d} ({len(val_df)/len(df):.1%})\")\n",
    "print(f\"  Test:  {len(test_df):5d} ({len(test_df)/len(df):.1%})\")\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(f\"  Train - Van Gogh: {train_df['is_van_gogh'].mean():.2%}\")\n",
    "print(f\"  Val   - Van Gogh: {val_df['is_van_gogh'].mean():.2%}\")\n",
    "print(f\"  Test  - Van Gogh: {test_df['is_van_gogh'].mean():.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwqo_KbRP-BS"
   },
   "source": [
    "## 4. Data Transforms & Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSzBqFT7P-BS"
   },
   "outputs": [],
   "source": [
    "# Define image transforms: resize to 224x224, normalize with ImageNet stats\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Training: data augmentation (random crop, flip, rotation, color jitter)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# Evaluation: no augmentation (consistent evaluation)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "print(\"Transforms defined ‚úì\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXFfgteAP-BS"
   },
   "outputs": [],
   "source": [
    "# Custom PyTorch Dataset class to load images from file paths\n",
    "class VanGoghDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        # Filter out files that don't exist\n",
    "        self.df = self.df[self.df['filepath'].apply(os.path.exists)].reset_index(drop=True)\n",
    "        if len(self.df) < len(df):\n",
    "            print(f\"‚ö†Ô∏è Warning: {len(df) - len(self.df)} files not found and removed\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        try:\n",
    "            image = Image.open(row[\"filepath\"]).convert(\"RGB\")\n",
    "            label = row[\"is_van_gogh\"]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {row['filepath']}: {e}\")\n",
    "            # Return a black image as fallback (shouldn't happen if we filtered)\n",
    "            image = Image.new('RGB', (224, 224), color='black')\n",
    "            label = row[\"is_van_gogh\"]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = VanGoghDataset(train_df, transform=train_transform)\n",
    "val_dataset = VanGoghDataset(val_df, transform=eval_transform)\n",
    "test_dataset = VanGoghDataset(test_df, transform=eval_transform)\n",
    "\n",
    "print(f\"Datasets: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bccCmJABP-BT"
   },
   "source": [
    "## 5. Training Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lk0j02iOP-BT"
   },
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch: forward pass, compute loss, backward pass, update weights\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\"batch_loss\": loss.item()})\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate: forward pass only, compute loss and accuracy\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "print(\"Training functions defined ‚úì\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeGOP6L1P-BT"
   },
   "source": [
    "## 6. Hyperparameter Tuning with Optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoopVVlgP-BT"
   },
   "outputs": [],
   "source": [
    "# Create model: supports both VGG19 and AlexNet (project requirement)\n",
    "def create_model(model_name='VGG19', freeze_features=True, dropout=0.5):\n",
    "    \"\"\"\n",
    "    Create model with binary classifier.\n",
    "    \n",
    "    Args:\n",
    "        model_name: 'VGG19' or 'AlexNet'\n",
    "        freeze_features: If True, freeze feature extractor (only train classifier)\n",
    "        dropout: Dropout rate for classifier (0.0 to 0.7)\n",
    "    \"\"\"\n",
    "    if model_name == 'VGG19':\n",
    "        model = models.vgg19(weights='IMAGENET1K_V1')\n",
    "        if freeze_features:\n",
    "            for param in model.features.parameters():\n",
    "                param.requires_grad = False\n",
    "        # Modify classifier: VGG19 classifier[6] is the last Linear layer\n",
    "        num_features = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(num_features, 2)  # Binary classification\n",
    "        )\n",
    "    elif model_name == 'AlexNet':\n",
    "        model = models.alexnet(weights='IMAGENET1K_V1')\n",
    "        if freeze_features:\n",
    "            for param in model.features.parameters():\n",
    "                param.requires_grad = False\n",
    "        # Modify classifier: AlexNet classifier[6] is the last Linear layer\n",
    "        num_features = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(num_features, 2)  # Binary classification\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective: try hyperparameters, train model, return validation accuracy\"\"\"\n",
    "    # Optuna suggests hyperparameters\n",
    "    model_name = trial.suggest_categorical(\"model_name\", [\"VGG19\", \"AlexNet\"])  # Project requirement: both models\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-4, log=True)  # Reduced max LR for stability (was 1e-2)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"AdamW\"])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64])  # Added 8 for smaller GPUs\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    momentum = trial.suggest_float(\"momentum\", 0.8, 0.99) if optimizer_name == \"SGD\" else 0.0\n",
    "    freeze_features = trial.suggest_categorical(\"freeze_features\", [True, False])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.7)  # Added dropout search\n",
    "\n",
    "    # Create DataLoaders, model, optimizer\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)  # Like Nir\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=\"VanGogh_Classifier\",\n",
    "        name=f\"trial_{trial.number}_{model_name}\",\n",
    "        config={\"model_name\": model_name, \"lr\": lr, \"optimizer\": optimizer_name, \"batch_size\": batch_size,\n",
    "                \"weight_decay\": weight_decay, \"momentum\": momentum,\n",
    "                \"freeze_features\": freeze_features, \"dropout\": dropout},\n",
    "        reinit=True\n",
    "    )\n",
    "\n",
    "    model = create_model(model_name=model_name, freeze_features=freeze_features, dropout=dropout)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "    if optimizer_name == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(trainable_params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"AdamW\":\n",
    "        optimizer = torch.optim.AdamW(trainable_params, lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(trainable_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Train for a few epochs (quick evaluation for hyperparameter search)\n",
    "    num_epochs = 3  # Reduced for faster trials (30-60 min total search time)\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = eval_one_epoch(model, val_loader, criterion, device)\n",
    "        best_val_acc = max(best_val_acc, val_acc)\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss,\n",
    "                   \"val_acc\": val_acc, \"best_val_acc\": best_val_acc})\n",
    "        trial.report(val_acc, epoch)\n",
    "        if trial.should_prune():  # Stop bad trials early\n",
    "            run.finish()\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    run.finish()\n",
    "    return best_val_acc\n",
    "\n",
    "print(\"Optuna objective function defined ‚úì (supports VGG19 and AlexNet)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6T2jcKusP-BU"
   },
   "outputs": [],
   "source": [
    "# Run hyperparameter search with Optuna\n",
    "# Project requirement: must take at least 30 minutes, max 60 minutes\n",
    "import optuna  # Ensure optuna is imported (in case cells run out of order)\n",
    "import time\n",
    "\n",
    "print(\"Starting hyperparameter search...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=2)\n",
    ")\n",
    "\n",
    "# Track start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Run optimization (max 60 minutes - will stop at timeout)\n",
    "study.optimize(objective, n_trials=10, timeout=3600, show_progress_bar=True)  # 10 trials for 30-60 min window\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_minutes = elapsed_time / 60\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYPERPARAMETER SEARCH COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚è±Ô∏è  Time taken: {elapsed_minutes:.2f} minutes ({elapsed_time:.0f} seconds)\")\n",
    "print(f\"üìä Completed trials: {len(study.trials)} / 10\")\n",
    "\n",
    "# Check project requirements\n",
    "if elapsed_minutes < 30:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: Search took only {elapsed_minutes:.2f} minutes!\")\n",
    "    print(\"   Project requires at least 30 minutes. Consider increasing n_trials.\")\n",
    "elif elapsed_minutes > 60:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: Search took {elapsed_minutes:.2f} minutes (exceeded 60 min limit)\")\n",
    "else:\n",
    "    print(f\"\\n‚úì Time requirement met: {elapsed_minutes:.2f} minutes (30-60 min range)\")\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {study.best_value:.4f}\")\n",
    "print(f\"\\nBest hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JM7K6ZYQP-BV"
   },
   "source": [
    "## 7. Train Final Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Y6f0EkbP-BV"
   },
   "outputs": [],
   "source": [
    "# Train final model with best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Training final model with best parameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Setup: DataLoaders, model, optimizer, scheduler\n",
    "final_train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True, num_workers=2)\n",
    "final_val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "final_model = create_model(\n",
    "    model_name=best_params.get('model_name', 'VGG19'),\n",
    "    freeze_features=best_params.get('freeze_features', False),\n",
    "    dropout=best_params.get('dropout', 0.5)\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "trainable_params = filter(lambda p: p.requires_grad, final_model.parameters())\n",
    "\n",
    "if best_params['optimizer'] == \"SGD\":\n",
    "    final_optimizer = torch.optim.SGD(trainable_params, lr=best_params['lr'],\n",
    "                                       momentum=best_params.get('momentum', 0.9),\n",
    "                                       weight_decay=best_params.get('weight_decay', 1e-4))\n",
    "elif best_params['optimizer'] == \"AdamW\":\n",
    "    final_optimizer = torch.optim.AdamW(trainable_params, lr=best_params['lr'],\n",
    "                                         weight_decay=best_params.get('weight_decay', 1e-4))\n",
    "else:\n",
    "    final_optimizer = torch.optim.Adam(trainable_params, lr=best_params['lr'],\n",
    "                                        weight_decay=best_params.get('weight_decay', 1e-4))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(final_optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "# Training loop with early stopping\n",
    "run = wandb.init(project=\"VanGogh_Classifier\", name=\"final_model_training\",\n",
    "                 config={**best_params, \"training_type\": \"final\"}, reinit=True)\n",
    "\n",
    "num_epochs = 15  # Reduced for time efficiency\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "patience = 3  # Early stopping patience\n",
    "epochs_without_improvement = 0\n",
    "train_losses, val_losses, val_accuracies = [], [], []\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(final_model, final_train_loader, final_optimizer, criterion, device)\n",
    "    val_loss, val_acc = eval_one_epoch(final_model, final_val_loader, criterion, device)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    scheduler.step(val_acc)\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss,\n",
    "               \"val_acc\": val_acc, \"lr\": final_optimizer.param_groups[0]['lr']})\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = {k: v.cpu().clone() for k, v in final_model.state_dict().items()}\n",
    "        epochs_without_improvement = 0\n",
    "        print(f\"  ‚úì New best model! (Val Acc: {best_val_acc:.4f})\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"\\nEarly stopping after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "final_model.load_state_dict(best_model_state)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "torch.save({'model_state_dict': best_model_state, 'best_params': best_params,\n",
    "            'best_val_acc': best_val_acc}, 'best_vangogh_classifier.pth')\n",
    "print(\"Model saved to 'best_vangogh_classifier.pth'\")\n",
    "\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-ko9yReP-BW"
   },
   "source": [
    "## 8. Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErWsvIAaP-BW"
   },
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(train_losses, label='Train Loss', color='#1f77b4', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Validation Loss', color='#ff7f0e', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(val_accuracies, label='Validation Accuracy', color='#2ca02c', linewidth=2, marker='o')\n",
    "axes[1].axhline(y=best_val_acc, color='r', linestyle='--', label=f'Best: {best_val_acc:.4f}')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Validation Accuracy', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: training_curves.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZs0WVCpP-BX"
   },
   "source": [
    "## 9. Test Set Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8M_t2gMP-BX"
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set (final performance metric)\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "final_model.eval()\n",
    "all_preds, all_labels, all_probs = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = final_model(images)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "\n",
    "all_preds, all_labels, all_probs = np.array(all_preds), np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "# Calculate metrics\n",
    "test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "test_precision = precision_score(all_labels, all_preds)\n",
    "test_recall = recall_score(all_labels, all_preds)\n",
    "test_f1 = f1_score(all_labels, all_preds)\n",
    "test_auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Metrics:\")\n",
    "print(f\"   Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"   Precision: {test_precision:.4f}\")\n",
    "print(f\"   Recall:    {test_recall:.4f}\")\n",
    "print(f\"   F1-Score:  {test_f1:.4f}\")\n",
    "print(f\"   AUC-ROC:   {test_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nüìã Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=['Not Van Gogh', 'Van Gogh']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-_b18W6P-BX"
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix and ROC Curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Not Van Gogh', 'Van Gogh'],\n",
    "            yticklabels=['Not Van Gogh', 'Van Gogh'], annot_kws={'size': 14})\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix', fontweight='bold')\n",
    "\n",
    "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "axes[1].plot(fpr, tpr, color='#1f77b4', linewidth=2, label=f'ROC (AUC = {test_auc:.4f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "axes[1].fill_between(fpr, tpr, alpha=0.2, color='#1f77b4')\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_roc.png', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: confusion_matrix_roc.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize False Positives (images predicted as Van Gogh but are not)\n",
    "# This is important for analysis in Part 3\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Find False Positives\n",
    "false_positives = []\n",
    "for idx, (pred, label, prob) in enumerate(zip(all_preds, all_labels, all_probs)):\n",
    "    if pred == 1 and label == 0:  # Predicted Van Gogh but is not\n",
    "        false_positives.append((idx, prob))\n",
    "\n",
    "# Sort by confidence (highest probability first)\n",
    "false_positives.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display top False Positives\n",
    "num_to_show = min(12, len(false_positives))\n",
    "if num_to_show > 0:\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    fig.suptitle('False Positives: Predicted as Van Gogh (but are not)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, (idx, prob) in enumerate(false_positives[:num_to_show]):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        \n",
    "        # Get image path from test dataset\n",
    "        image_path = test_df.iloc[idx]['filepath']\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        axes[row, col].imshow(image)\n",
    "        axes[row, col].set_title(f'Conf: {prob:.3f}\\n{test_df.iloc[idx][\"artist\"]}', fontsize=10)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(num_to_show, 12):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('false_positives.png', dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"Saved: false_positives.png\")\n",
    "    print(f\"\\nTotal False Positives: {len(false_positives)} / {len(all_preds)}\")\n",
    "else:\n",
    "    print(\"No False Positives found!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeXTrTaHP-Bf"
   },
   "source": [
    "## 10. Final Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1H9E0j2oP-Bi"
   },
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"=\"*60)\n",
    "print(\"PART A - VAN GOGH CLASSIFIER - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìÅ Dataset:\")\n",
    "print(f\"   Total: {len(df)} | Van Gogh: {df['is_van_gogh'].sum()} | Other: {len(df)-df['is_van_gogh'].sum()}\")\n",
    "print(f\"   Split: 70% train / 15% val / 15% test\")\n",
    "\n",
    "print(\"\\nüîß Best Hyperparameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "\n",
    "print(\"\\nüìä Performance:\")\n",
    "print(f\"   Best Val Acc:  {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "print(f\"   Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"   Test F1-Score: {test_f1:.4f}\")\n",
    "print(f\"   Test AUC-ROC:  {test_auc:.4f}\")\n",
    "\n",
    "print(\"\\nüíæ Saved Files:\")\n",
    "print(\"   best_vangogh_classifier.pth\")\n",
    "print(\"   training_curves.png\")\n",
    "print(\"   confusion_matrix_roc.png\")\n",
    "\n",
    "print(\"\\n‚úÖ Part A Complete!\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
