{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8h8XmJVfP-A8"
   },
   "source": [
    "# Part A - Van Gogh Painting Classifier\n",
    "\n",
    "## Deep Learning Project - Tel Aviv University\n",
    "\n",
    "This notebook implements a binary classifier to identify Van Gogh paintings using transfer learning with VGG19.\n",
    "\n",
    "### Overview:\n",
    "1. **Load Data**: Read pre-prepared CSV file with image paths and labels\n",
    "2. **Split Dataset**: 70% train, 15% validation, 15% test\n",
    "3. **Data Augmentation**: Apply transforms for training robustness\n",
    "4. **Model**: VGG19 pre-trained on ImageNet, fine-tuned for binary classification\n",
    "5. **Hyperparameter Tuning**: Use Optuna to find best parameters\n",
    "6. **Training**: Train final model with best hyperparameters\n",
    "7. **Evaluation**: Test set metrics and visualizations\n",
    "\n",
    "### Requirements:\n",
    "- Google Colab with GPU runtime\n",
    "- `post_impressionism_data.csv` file (created by Get_Post_Impressionism_Data.ipynb)\n",
    "- Weights & Biases account for experiment tracking\n",
    "\n",
    "---\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iQGUIlOP-A-"
   },
   "source": [
    "## 1. Environment Setup\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQqOcR7cP-A_",
    "outputId": "9d920786-7eeb-469c-8298-7ab7119792f4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check if running on Google Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running on Google Colab\")\n",
    "else:\n",
    "    print(\"Running locally\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-wFi03lP-BA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages: optuna (hyperparameter tuning), wandb (experiment tracking)\n",
    "%pip install -q optuna wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zj0h6Pw0P-BC"
   },
   "source": [
    "## 2. Import Libraries\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8aoyWL1P-BF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning and logging\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Hardware Benchmark Setup (Required by instructions - Page 7)\n",
    "print(\"=\"*60)\n",
    "print(\"HARDWARE BENCHMARK INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Python Version: {platform.python_version()}\")\n",
    "print(f\"Machine Name: {platform.node()}\")\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory Total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NTjxVeHP-BO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup device (GPU/CPU) - GPU is much faster for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cpu':\n",
    "    print(\"WARNING: Running on CPU. Enable GPU in Colab: Runtime -> Change runtime type -> GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CPU vs GPU Benchmark (Required by instruction 1.34)\n",
    "# Measure time for 20 forward+backward iterations on both CPU and GPU\n",
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CPU vs GPU BENCHMARK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Machine name: {platform.node()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a dummy model for benchmarking\n",
    "dummy_model = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64, 2)\n",
    ")\n",
    "dummy_input = torch.randn(8, 3, 224, 224)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dummy_labels = torch.randint(0, 2, (8,))\n",
    "\n",
    "# CPU Benchmark\n",
    "print(\"\\nRunning CPU benchmark (20 iterations)...\")\n",
    "cpu_model = dummy_model.to('cpu')\n",
    "cpu_input = dummy_input.to('cpu')\n",
    "cpu_labels = dummy_labels.to('cpu')\n",
    "cpu_optimizer = torch.optim.Adam(cpu_model.parameters(), lr=0.001)\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(20):\n",
    "    cpu_optimizer.zero_grad()\n",
    "    outputs = cpu_model(cpu_input)\n",
    "    loss = criterion(outputs, cpu_labels)\n",
    "    loss.backward()\n",
    "    cpu_optimizer.step()\n",
    "cpu_time = time.time() - start_time\n",
    "\n",
    "print(f\"CPU time: {cpu_time:.4f} seconds ({cpu_time/20*1000:.2f} ms per iteration)\")\n",
    "\n",
    "# GPU Benchmark (if available)\n",
    "if torch.cuda.is_available():\n",
    "    # Print machine name and GPU model before results (for documentation)\n",
    "    print(f\"\\nMachine Name: {platform.node()}\")\n",
    "    print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"\\nRunning GPU benchmark (20 iterations)...\")\n",
    "    gpu_model = dummy_model.to('cuda')\n",
    "    gpu_input = dummy_input.to('cuda')\n",
    "    gpu_labels = dummy_labels.to('cuda')\n",
    "    gpu_optimizer = torch.optim.Adam(gpu_model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        gpu_optimizer.zero_grad()\n",
    "        outputs = gpu_model(gpu_input)\n",
    "        loss = criterion(outputs, gpu_labels)\n",
    "        loss.backward()\n",
    "        gpu_optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for _ in range(20):\n",
    "        gpu_optimizer.zero_grad()\n",
    "        outputs = gpu_model(gpu_input)\n",
    "        loss = criterion(outputs, gpu_labels)\n",
    "        loss.backward()\n",
    "        gpu_optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    gpu_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"GPU time: {gpu_time:.4f} seconds ({gpu_time/20*1000:.2f} ms per iteration)\")\n",
    "    print(f\"\\nSpeedup: {cpu_time/gpu_time:.2f}x faster on GPU\")\n",
    "else:\n",
    "    print(\"\\nGPU not available, skipping GPU benchmark\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96eF9IOzP-BP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Login to Weights & Biases for experiment tracking\n",
    "wandb.login(key=\"16d1bc863b28f81253ac0ee253b453393791a7e1\")\n",
    "print(\"Logged in to Weights & Biases\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PylwidTjP-BP"
   },
   "source": [
    "## 3. Data Preparation\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Scan Post_Impressionism directory and create metadata CSV\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Find Post_Impressionism directory (works in both Kaggle and Colab)\n",
    "possible_dirs = [\n",
    "    \"/kaggle/input/Post_Impressionism\",  # Kaggle\n",
    "    \"/content/data/Post_Impressionism\",          # Colab\n",
    "    \"/content/Post_Impressionism\",               # Colab alternative\n",
    "    \"/content/wikiart/Post_Impressionism\",       # Colab alternative\n",
    "]\n",
    "\n",
    "base_dir = None\n",
    "for dir_path in possible_dirs:\n",
    "    if os.path.exists(dir_path):\n",
    "        base_dir = dir_path\n",
    "        break\n",
    "\n",
    "if base_dir is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Post_Impressionism directory not found!\\n\"\n",
    "        f\"   Checked: {possible_dirs}\\n\"\n",
    "        f\"   Please download images to one of these locations.\"\n",
    "    )\n",
    "\n",
    "print(f\"Found images in: {base_dir}\")\n",
    "\n",
    "# Scan directory and create DataFrame (like Nir)\n",
    "records = []\n",
    "for fname in os.listdir(base_dir):\n",
    "    if not fname.lower().endswith((\".jpg\", \".png\")):\n",
    "        continue\n",
    "    \n",
    "    artist = fname.split(\"_\")[0]  # Extract artist name from filename\n",
    "    \n",
    "    records.append({\n",
    "        \"filepath\": os.path.join(base_dir, fname),\n",
    "        \"filename\": fname,\n",
    "        \"artist\": artist,\n",
    "        \"is_van_gogh\": 1 if \"van-gogh\" in artist.lower() else 0\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save metadata to CSV\n",
    "csv_output_path = \"/kaggle/working/post_impressionism_data.csv\" if os.path.exists(\"/kaggle\") else (\"/content/post_impressionism_data.csv\" if IN_COLAB else \"post_impressionism_data.csv\")\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "print(f\"Saved metadata CSV: {csv_output_path}\")\n",
    "\n",
    "print(f\"\\nLoaded: {len(df)} images\")\n",
    "print(f\"  Van Gogh: {df['is_van_gogh'].sum()}\")\n",
    "print(f\"  Other: {len(df) - df['is_van_gogh'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLShT63aP-BR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split dataset: 70% train, 15% validation, 15% test (stratified to maintain class balance)\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.3, stratify=df[\"is_van_gogh\"], random_state=SEED\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df[\"is_van_gogh\"], random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"  Train: {len(train_df):5d} ({len(train_df)/len(df):.1%})\")\n",
    "print(f\"  Val:   {len(val_df):5d} ({len(val_df)/len(df):.1%})\")\n",
    "print(f\"  Test:  {len(test_df):5d} ({len(test_df)/len(df):.1%})\")\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(f\"  Train - Van Gogh: {train_df['is_van_gogh'].mean():.2%}\")\n",
    "print(f\"  Val   - Van Gogh: {val_df['is_van_gogh'].mean():.2%}\")\n",
    "print(f\"  Test  - Van Gogh: {test_df['is_van_gogh'].mean():.2%}\")\n",
    "\n",
    "# Save dataset splits for later use (so data preparation doesn't need to be rerun)\n",
    "import pickle\n",
    "import os\n",
    "splits_save_path = 'dataset_splits.pkl'\n",
    "with open(splits_save_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'train_df': train_df,\n",
    "        'val_df': val_df,\n",
    "        'test_df': test_df\n",
    "    }, f)\n",
    "print(f\"\\nDataset splits saved to '{splits_save_path}' (can skip data preparation next time)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwqo_KbRP-BS"
   },
   "source": [
    "## 4. Data Transforms & Dataset Class\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSzBqFT7P-BS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define image transforms: resize to 224x224, normalize with ImageNet stats\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Training: data augmentation (random crop, flip, rotation, color jitter)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# Evaluation: no augmentation (consistent evaluation)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "print(\"Transforms defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXFfgteAP-BS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Custom PyTorch Dataset class to load images from file paths\n",
    "class VanGoghDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        # Filter out files that don't exist\n",
    "        self.df = self.df[self.df['filepath'].apply(os.path.exists)].reset_index(drop=True)\n",
    "        if len(self.df) < len(df):\n",
    "            print(f\"Warning: {len(df) - len(self.df)} files not found and removed\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        try:\n",
    "            image = Image.open(row[\"filepath\"]).convert(\"RGB\")\n",
    "            label = row[\"is_van_gogh\"]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {row['filepath']}: {e}\")\n",
    "            # Return a black image as fallback (shouldn't happen if we filtered)\n",
    "            image = Image.new('RGB', (224, 224), color='black')\n",
    "            label = row[\"is_van_gogh\"]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "\n",
    "# Load dataset splits if they don't exist (in case data preparation wasn't run)\n",
    "if 'train_df' not in globals() or 'val_df' not in globals() or 'test_df' not in globals():\n",
    "    import pickle\n",
    "    import os\n",
    "    splits_save_path = 'dataset_splits.pkl'\n",
    "    if os.path.exists(splits_save_path):\n",
    "        print(f\"Loading dataset splits from {splits_save_path}...\")\n",
    "        with open(splits_save_path, 'rb') as f:\n",
    "            splits_data = pickle.load(f)\n",
    "        train_df = splits_data['train_df']\n",
    "        val_df = splits_data['val_df']\n",
    "        test_df = splits_data['test_df']\n",
    "        print(f\"Loaded dataset splits: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
    "    else:\n",
    "        raise NameError(\n",
    "            f\"Dataset splits not found and '{splits_save_path}' doesn't exist!\\n\"\n",
    "            \"   Please run data preparation steps (Steps 3-4) first.\"\n",
    "        )\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = VanGoghDataset(train_df, transform=train_transform)\n",
    "val_dataset = VanGoghDataset(val_df, transform=eval_transform)\n",
    "test_dataset = VanGoghDataset(test_df, transform=eval_transform)\n",
    "\n",
    "print(f\"Datasets: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bccCmJABP-BT"
   },
   "source": [
    "## 5. Training Functions\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lk0j02iOP-BT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, max_grad_norm=1.0):\n",
    "    \"\"\"Train for one epoch: forward pass, compute loss, backward pass, update weights\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        # Gradient clipping for training stability\n",
    "        if max_grad_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # Note: batch_loss logging removed to avoid interfering with step=epoch alignment\n",
    "        # Batch-level metrics are noisy and can cause step counter confusion\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate: forward pass only, compute loss and accuracy\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "print(\"Training functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeGOP6L1P-BT"
   },
   "source": [
    "## 6. Hyperparameter Tuning with Optuna\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoopVVlgP-BT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create model: supports both VGG19 and AlexNet (project requirement)\n",
    "# Smart Fine-tuning: When freeze_features=False, unfreeze only top layers to prevent overfitting\n",
    "def create_model(model_name='VGG19', freeze_features=True, dropout=0.5):\n",
    "    \"\"\"\n",
    "    Create model with binary classifier.\n",
    "    Implements Smart Fine-tuning: When freeze_features=False, unfreezes only top layers.\n",
    "    \n",
    "    Args:\n",
    "        model_name: 'VGG19' or 'AlexNet'\n",
    "        freeze_features: If True, freeze feature extractor (only train classifier)\n",
    "                        If False, unfreeze only top layers (Smart Fine-tuning)\n",
    "        dropout: Dropout rate for classifier (0.0 to 0.7)\n",
    "    \"\"\"\n",
    "    if model_name == 'VGG19':\n",
    "        model = models.vgg19(weights='IMAGENET1K_V1')\n",
    "        # First, freeze all feature layers by default\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Smart Fine-tuning: If freeze_features=False, unfreeze only the last 8 layers\n",
    "        if not freeze_features:\n",
    "            # VGG19 features has 36 layers (0-35), unfreeze last 8 layers (28-35)\n",
    "            # This corresponds to the top convolutional blocks\n",
    "            for i in range(28, 36):\n",
    "                for param in model.features[i].parameters():\n",
    "                    param.requires_grad = True\n",
    "        \n",
    "        # Modify classifier: VGG19 classifier[6] is the last Linear layer\n",
    "        # Classifier is always trainable\n",
    "        num_features = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(num_features, 2)  # Binary classification\n",
    "        )\n",
    "    elif model_name == 'AlexNet':\n",
    "        model = models.alexnet(weights='IMAGENET1K_V1')\n",
    "        # First, freeze all feature layers by default\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Smart Fine-tuning: If freeze_features=False, unfreeze only the last 2 layers\n",
    "        # AlexNet features has 12 layers (0-11), unfreeze last 2 layers (10-11)\n",
    "        if not freeze_features:\n",
    "            for i in range(10, 12):\n",
    "                for param in model.features[i].parameters():\n",
    "                    param.requires_grad = True\n",
    "        \n",
    "        # Modify classifier: AlexNet classifier[6] is the last Linear layer\n",
    "        # Classifier is always trainable\n",
    "        num_features = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(num_features, 2)  # Binary classification\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective: try hyperparameters, train model, return validation loss (minimize)\"\"\"\n",
    "    # Optuna suggests hyperparameters\n",
    "    model_name = trial.suggest_categorical(\"model_name\", [\"VGG19\", \"AlexNet\"])  # Project requirement: both models\n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-4, log=True)  # Lower LR range for fine-tuning stability\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"AdamW\"])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64])  # Added 8 for smaller GPUs\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    momentum = trial.suggest_float(\"momentum\", 0.8, 0.99) if optimizer_name == \"SGD\" else 0.0\n",
    "    freeze_features = trial.suggest_categorical(\"freeze_features\", [True, False])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.7)  # Added dropout search\n",
    "\n",
    "    # Create DataLoaders, model, optimizer\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)  # Like Nir\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=\"VanGogh_Classifier\",\n",
    "        name=f\"trial_{trial.number}_{model_name}\",\n",
    "        config={\"model_name\": model_name, \"lr\": lr, \"optimizer\": optimizer_name, \"batch_size\": batch_size,\n",
    "                \"weight_decay\": weight_decay, \"momentum\": momentum,\n",
    "                \"freeze_features\": freeze_features, \"dropout\": dropout},\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    # Define epoch as step metric to ensure X-axis starts at 0 for each trial\n",
    "    run.define_metric(\"epoch\", hidden=True)\n",
    "    run.define_metric(\"train_loss\", step_metric=\"epoch\")\n",
    "    run.define_metric(\"val_loss\", step_metric=\"epoch\")\n",
    "    run.define_metric(\"val_acc\", step_metric=\"epoch\")\n",
    "    run.define_metric(\"best_val_loss\", step_metric=\"epoch\")\n",
    "\n",
    "    model = create_model(model_name=model_name, freeze_features=freeze_features, dropout=dropout)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "    if optimizer_name == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(trainable_params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"AdamW\":\n",
    "        optimizer = torch.optim.AdamW(trainable_params, lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(trainable_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Train for a few epochs (quick evaluation for hyperparameter search)\n",
    "    num_epochs = 3  # Reduced for faster trials (30-60 min total search time)\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = eval_one_epoch(model, val_loader, criterion, device)\n",
    "        best_val_loss = min(best_val_loss, val_loss)\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss,\n",
    "                   \"val_acc\": val_acc, \"best_val_loss\": best_val_loss}, step=epoch)\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():  # Stop bad trials early\n",
    "            run.finish()\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    run.finish()\n",
    "    return best_val_loss\n",
    "\n",
    "print(\"Optuna objective function defined (supports VGG19 and AlexNet, minimizes validation loss)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6T2jcKusP-BU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run hyperparameter search with Optuna\n",
    "# Project requirement: must take at least 30 minutes, max 60 minutes\n",
    "# Objective: minimize validation loss\n",
    "import optuna  # Ensure optuna is imported (in case cells run out of order)\n",
    "import time\n",
    "\n",
    "print(\"Starting hyperparameter search...\")\n",
    "print(\"=\"*60)\n",
    "print(\"Objective: Minimize validation loss\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=2)\n",
    ")\n",
    "\n",
    "# Track start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Run optimization (max 60 minutes - will stop at timeout)\n",
    "study.optimize(objective, n_trials=10, timeout=3600, show_progress_bar=True)  # 10 trials for 30-60 min window\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_minutes = elapsed_time / 60\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYPERPARAMETER SEARCH COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTime taken: {elapsed_minutes:.2f} minutes ({elapsed_time:.0f} seconds)\")\n",
    "print(f\"Completed trials: {len(study.trials)} / 10\")\n",
    "\n",
    "# Check project requirements\n",
    "if elapsed_minutes < 30:\n",
    "    print(f\"\\nWARNING: Search took only {elapsed_minutes:.2f} minutes!\")\n",
    "    print(\"   Project requires at least 30 minutes. Consider increasing n_trials.\")\n",
    "elif elapsed_minutes > 60:\n",
    "    print(f\"\\nWARNING: Search took {elapsed_minutes:.2f} minutes (exceeded 60 min limit)\")\n",
    "else:\n",
    "    print(f\"\\nTime requirement met: {elapsed_minutes:.2f} minutes (30-60 min range)\")\n",
    "\n",
    "print(f\"\\nBest validation loss: {study.best_value:.4f}\")\n",
    "print(f\"\\nBest hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save study results for later use (so step 7 can run independently)\n",
    "import pickle\n",
    "study_save_path = 'optuna_study_results.pkl'\n",
    "with open(study_save_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'best_params': study.best_params,\n",
    "        'best_value': study.best_value,\n",
    "        'n_trials': len(study.trials)\n",
    "    }, f)\n",
    "print(f\"\\nStudy results saved to '{study_save_path}' (can skip step 6 next time)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JM7K6ZYQP-BV"
   },
   "source": [
    "## 7. Train Final Model\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GPU Utilization Monitoring (Required for appendix)\n",
    "# Run nvidia-smi to show GPU utilization and memory usage\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GPU UTILIZATION (nvidia-smi)\")\n",
    "print(\"=\"*60)\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=5)\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"Stderr:\", result.stderr, file=sys.stderr)\n",
    "except FileNotFoundError:\n",
    "    print(\"nvidia-smi not found. This is expected if running on CPU or in some environments.\")\n",
    "    print(\"   GPU monitoring will be skipped.\")\n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"nvidia-smi timed out.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not run nvidia-smi: {e}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Y6f0EkbP-BV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train final model with best hyperparameters\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load best_params if study doesn't exist (in case step 6 wasn't run)\n",
    "if 'study' not in globals() or study is None:\n",
    "    study_save_path = 'optuna_study_results.pkl'\n",
    "    if os.path.exists(study_save_path):\n",
    "        print(f\"Loading hyperparameters from {study_save_path}...\")\n",
    "        with open(study_save_path, 'rb') as f:\n",
    "            study_data = pickle.load(f)\n",
    "        best_params = study_data['best_params']\n",
    "        print(f\"Loaded hyperparameters (Best val loss: {study_data['best_value']:.4f})\")\n",
    "    else:\n",
    "        raise NameError(\n",
    "            f\"'study' not found and '{study_save_path}' doesn't exist!\\n\"\n",
    "            \"   Please run Step 6 (Hyperparameter Tuning) first.\"\n",
    "        )\n",
    "else:\n",
    "    best_params = study.best_params\n",
    "\n",
    "print(\"\\nTraining final model with best parameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Anti-overfitting improvements: increase dropout and weight decay\n",
    "# Optuna found dropout ~0.29, but we have overfitting - increase to 0.4 for better regularization\n",
    "original_dropout = best_params.get('dropout', 0.5)\n",
    "improved_dropout = max(0.4, min(0.5, original_dropout + 0.1))  # Increase by 0.1, cap at 0.5\n",
    "print(f\"\\nAnti-overfitting adjustments:\")\n",
    "print(f\"   Dropout: {original_dropout:.4f} -> {improved_dropout:.4f}\")\n",
    "\n",
    "original_weight_decay = best_params.get('weight_decay', 1e-4)\n",
    "improved_weight_decay = original_weight_decay * 1.25  # Increase by 25% for more regularization\n",
    "print(f\"   Weight decay: {original_weight_decay:.6f} \u2192 {improved_weight_decay:.6f}\")\n",
    "\n",
    "# Setup: DataLoaders, model, optimizer, scheduler\n",
    "# Stability improvement: Manually set batch_size to 32 for final training (as per instructions)\n",
    "# This reduces 'zigzag' effect in loss curves, regardless of Optuna suggestion\n",
    "final_batch_size = 32  # Fixed batch size for stability (use 16 if memory is an issue)\n",
    "print(f\"\\nBatch size: Optuna suggested {best_params.get('batch_size', 'N/A')}, using {final_batch_size} for stability\")\n",
    "final_train_loader = DataLoader(train_dataset, batch_size=final_batch_size, shuffle=True, num_workers=2)\n",
    "final_val_loader = DataLoader(val_dataset, batch_size=final_batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "trained_classifier = create_model(\n",
    "    model_name=best_params.get('model_name', 'VGG19'),\n",
    "    freeze_features=best_params.get('freeze_features', False),\n",
    "    dropout=improved_dropout  # Use improved dropout\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "trainable_params = filter(lambda p: p.requires_grad, trained_classifier.parameters())\n",
    "\n",
    "if best_params['optimizer'] == \"SGD\":\n",
    "    final_optimizer = torch.optim.SGD(trainable_params, lr=best_params['lr'],\n",
    "                                       momentum=best_params.get('momentum', 0.9),\n",
    "                                       weight_decay=improved_weight_decay)  # Use improved weight decay\n",
    "elif best_params['optimizer'] == \"AdamW\":\n",
    "    final_optimizer = torch.optim.AdamW(trainable_params, lr=best_params['lr'],\n",
    "                                         weight_decay=improved_weight_decay)  # Use improved weight decay\n",
    "else:\n",
    "    final_optimizer = torch.optim.Adam(trainable_params, lr=best_params['lr'],\n",
    "                                        weight_decay=improved_weight_decay)  # Use improved weight decay\n",
    "\n",
    "# Improved learning rate scheduler: ReduceLROnPlateau with better settings\n",
    "# Use min_lr to prevent LR from going too low, and smaller factor for smoother decay\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    final_optimizer, mode='max', factor=0.5, patience=2, \n",
    "    min_lr=1e-6 , threshold=0.001\n",
    ")\n",
    "\n",
    "# Training loop with early stopping\n",
    "run = wandb.init(project=\"VanGogh_Classifier\", name=\"trained_classifier_training\",\n",
    "                 config={**best_params, \"training_type\": \"final\", \"gradient_clipping\": True, \"final_batch_size\": final_batch_size}, reinit=True)\n",
    "\n",
    "# Define epoch as step metric to ensure X-axis starts at 0 for final training\n",
    "run.define_metric(\"epoch\", hidden=True)\n",
    "run.define_metric(\"train_loss\", step_metric=\"epoch\")\n",
    "run.define_metric(\"val_loss\", step_metric=\"epoch\")\n",
    "run.define_metric(\"val_acc\", step_metric=\"epoch\")\n",
    "run.define_metric(\"lr\", step_metric=\"epoch\")\n",
    "\n",
    "num_epochs = 15  # Reduced for time efficiency\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "patience = 5  # Increased early stopping patience (was 3) to allow more recovery\n",
    "epochs_without_improvement = 0\n",
    "train_losses, val_losses, val_accuracies = [], [], []\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\"*60)\n",
    "print(\"Improvements applied:\")\n",
    "print(\"   - Gradient clipping (max_norm=1.0) for stability\")\n",
    "print(\"   - Increased early stopping patience (3->5 epochs)\")\n",
    "print(\"   - Improved LR scheduler (min_lr, threshold)\")\n",
    "print(\"   - Increased dropout for better regularization\")\n",
    "print(\"   - Increased weight decay for more L2 regularization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(trained_classifier, final_train_loader, final_optimizer, criterion, device, max_grad_norm=1.0)\n",
    "    val_loss, val_acc = eval_one_epoch(trained_classifier, final_val_loader, criterion, device)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    scheduler.step(val_acc)\n",
    "    current_lr = final_optimizer.param_groups[0]['lr']\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss,\n",
    "               \"val_acc\": val_acc, \"lr\": current_lr}, step=epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | LR: {current_lr:.2e}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = {k: v.cpu().clone() for k, v in trained_classifier.state_dict().items()}\n",
    "        epochs_without_improvement = 0\n",
    "        print(f\"  New best model! (Val Acc: {best_val_acc:.4f})\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"\\nEarly stopping after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "trained_classifier.load_state_dict(best_model_state)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Save model and training history for later use (so you don't need to retrain)\n",
    "torch.save({\n",
    "    'model_state_dict': best_model_state, \n",
    "    'best_params': best_params,\n",
    "    'best_val_acc': best_val_acc,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accuracies': val_accuracies\n",
    "}, 'best_vangogh_classifier.pth')\n",
    "print(\"Model and training history saved to 'best_vangogh_classifier.pth'\")\n",
    "\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-ko9yReP-BW"
   },
   "source": [
    "## 8. Visualizations\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErWsvIAaP-BW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt  # Ensure matplotlib is imported\n",
    "import os\n",
    "import torch  # Import torch for loading checkpoint\n",
    "\n",
    "# Load training history if variables don't exist (in case kernel was restarted)\n",
    "if 'train_losses' not in globals() or 'val_losses' not in globals() or 'val_accuracies' not in globals() or 'best_val_acc' not in globals():\n",
    "    checkpoint_path = 'best_vangogh_classifier.pth'\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading training history from {checkpoint_path}...\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        train_losses = checkpoint.get('train_losses', [])\n",
    "        val_losses = checkpoint.get('val_losses', [])\n",
    "        val_accuracies = checkpoint.get('val_accuracies', [])\n",
    "        best_val_acc = checkpoint.get('best_val_acc', 0.0)\n",
    "        print(f\"Loaded training history: {len(train_losses)} epochs, Best Val Acc: {best_val_acc:.4f}\")\n",
    "    else:\n",
    "        raise NameError(\n",
    "            f\"Training variables not found and checkpoint file '{checkpoint_path}' doesn't exist!\\n\"\n",
    "            \"   Please run Step 7 (Train Final Model) first before plotting.\"\n",
    "        )\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(train_losses, label='Train Loss', color='#1f77b4', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Validation Loss', color='#ff7f0e', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(val_accuracies, label='Validation Accuracy', color='#2ca02c', linewidth=2, marker='o')\n",
    "axes[1].axhline(y=best_val_acc, color='r', linestyle='--', label=f'Best: {best_val_acc:.4f}')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Validation Accuracy', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: training_curves.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZs0WVCpP-BX"
   },
   "source": [
    "## 9. Test Set Evaluation\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8M_t2gMP-BX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set (final performance metric)\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Load best_params if not available (needed for batch_size)\n",
    "if 'best_params' not in globals():\n",
    "    checkpoint_path = 'best_vangogh_classifier.pth'\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        best_params = checkpoint.get('best_params')\n",
    "        if best_params:\n",
    "            print(f\"Loaded best_params from checkpoint\")\n",
    "    if 'best_params' not in globals() or best_params is None:\n",
    "        import pickle\n",
    "        study_save_path = 'optuna_study_results.pkl'\n",
    "        if os.path.exists(study_save_path):\n",
    "            with open(study_save_path, 'rb') as f:\n",
    "                study_data = pickle.load(f)\n",
    "            best_params = study_data['best_params']\n",
    "            print(f\"Loaded best_params from study file\")\n",
    "        else:\n",
    "            raise NameError(\n",
    "                \"Cannot find best_params!\\n\"\n",
    "                \"   Please run Step 6 (Hyperparameter Tuning) or Step 7 (Train Final Model) first.\"\n",
    "            )\n",
    "\n",
    "# Check if trained_classifier exists (step 7 must be run for evaluation)\n",
    "if 'trained_classifier' not in globals():\n",
    "    raise NameError(\n",
    "        \"'trained_classifier' not found!\\n\"\n",
    "        \"   Please run Step 7 (Train Final Model) first before evaluation.\\n\"\n",
    "        \"   The model needs to be created and trained in step 7.\"\n",
    "    )\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "trained_classifier.eval()\n",
    "all_preds, all_labels, all_probs = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = trained_classifier(images)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "\n",
    "all_preds, all_labels, all_probs = np.array(all_preds), np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "# Calculate metrics\n",
    "test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "test_precision = precision_score(all_labels, all_preds)\n",
    "test_recall = recall_score(all_labels, all_preds)\n",
    "test_f1 = f1_score(all_labels, all_preds)\n",
    "test_auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"   Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"   Precision: {test_precision:.4f}\")\n",
    "print(f\"   Recall:    {test_recall:.4f}\")\n",
    "print(f\"   F1-Score:  {test_f1:.4f}\")\n",
    "print(f\"   AUC-ROC:   {test_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=['Not Van Gogh', 'Van Gogh']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-_b18W6P-BX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix and ROC Curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Not Van Gogh', 'Van Gogh'],\n",
    "            yticklabels=['Not Van Gogh', 'Van Gogh'], annot_kws={'size': 14})\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix', fontweight='bold')\n",
    "\n",
    "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "axes[1].plot(fpr, tpr, color='#1f77b4', linewidth=2, label=f'ROC (AUC = {test_auc:.4f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "axes[1].fill_between(fpr, tpr, alpha=0.2, color='#1f77b4')\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_roc.png', dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: confusion_matrix_roc.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualize False Positives (images predicted as Van Gogh but are not)\n",
    "# This is important for analysis in Part 3\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Find False Positives\n",
    "false_positives = []\n",
    "for idx, (pred, label, prob) in enumerate(zip(all_preds, all_labels, all_probs)):\n",
    "    if pred == 1 and label == 0:  # Predicted Van Gogh but is not\n",
    "        false_positives.append((idx, prob))\n",
    "\n",
    "# Sort by confidence (highest probability first)\n",
    "false_positives.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display top False Positives\n",
    "num_to_show = min(12, len(false_positives))\n",
    "if num_to_show > 0:\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    fig.suptitle('False Positives: Predicted as Van Gogh (but are not)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, (idx, prob) in enumerate(false_positives[:num_to_show]):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        \n",
    "        # Get image path from test dataset\n",
    "        image_path = test_df.iloc[idx]['filepath']\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        axes[row, col].imshow(image)\n",
    "        axes[row, col].set_title(f'Conf: {prob:.3f}\\n{test_df.iloc[idx][\"artist\"]}', fontsize=10)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(num_to_show, 12):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('false_positives.png', dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"Saved: false_positives.png\")\n",
    "    print(f\"\\nTotal False Positives: {len(false_positives)} / {len(all_preds)}\")\n",
    "else:\n",
    "    print(\"No False Positives found!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeXTrTaHP-Bf"
   },
   "source": [
    "## 10. Final Summary\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1H9E0j2oP-Bi",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"=\"*60)\n",
    "print(\"PART A - VAN GOGH CLASSIFIER - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nDataset:\")\n",
    "print(f\"   Total: {len(df)} | Van Gogh: {df['is_van_gogh'].sum()} | Other: {len(df)-df['is_van_gogh'].sum()}\")\n",
    "print(f\"   Split: 70% train / 15% val / 15% test\")\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "\n",
    "print(\"\\nPerformance:\")\n",
    "print(f\"   Best Val Acc:  {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "print(f\"   Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"   Test F1-Score: {test_f1:.4f}\")\n",
    "print(f\"   Test AUC-ROC:  {test_auc:.4f}\")\n",
    "\n",
    "print(\"\\nSaved Files:\")\n",
    "print(\"   best_vangogh_classifier.pth\")\n",
    "print(\"   training_curves.png\")\n",
    "print(\"   confusion_matrix_roc.png\")\n",
    "\n",
    "print(\"\\nPart A Complete!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# PART B - NEURAL STYLE TRANSFER\n",
    "# ============================================================\n",
    "\n",
    "This section implements style transfer using the trained classifier from Part A.\n",
    "The `trained_classifier` from Part A will be used as the judge for evaluating style transfer quality."
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model from Part A as judge\n",
    "# The trained_classifier from Part A is already trained and ready to use\n",
    "\n",
    "if \"trained_classifier\" not in globals():\n",
    "    raise NameError(\n",
    "        \"trained_classifier not found! Please run Part A first.\\n\"\n",
    "        \"The model needs to be trained in Part A before using it in Part B.\"\n",
    "    )\n",
    "\n",
    "# Use trained_classifier as the judge\n",
    "judge_model = trained_classifier\n",
    "judge_model.eval()  # Ensure it's in eval mode\n",
    "judge_model_name = best_params.get(\"model_name\", \"VGG19\") if \"best_params\" in globals() else \"VGG19\"\n",
    "\n",
    "print(f\"Using {judge_model_name} from Part A as judge for hyperparameter search\")\n",
    "print(f\"Model validation accuracy: {best_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Part 2A: Style Transfer Function\n",
    "\n",
    "This section implements the generic style transfer function (from Nir_part_B)\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image loading and preprocessing\n",
    "imsize = 512 if torch.cuda.is_available() else 128\n",
    "\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize((imsize, imsize)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def image_loader(image_path):\n",
    "    \"\"\"Load and preprocess an image\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "def imshow(tensor, title=None, save_path=None):\n",
    "    \"\"\"Display a tensor as an image\"\"\"\n",
    "    image = tensor.cpu().clone().detach().squeeze(0)\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    )\n",
    "    image = inv_normalize(image)\n",
    "    image = transforms.ToPILImage()(image.clamp(0, 1))\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if save_path:\n",
    "        image.save(save_path)\n",
    "    \n",
    "    plt.show()\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for Gram matrix (used in style loss)\n",
    "def get_gram_matrix(tensor):\n",
    "    \"\"\"Compute Gram matrix for style loss\"\"\"\n",
    "    b, c, h, w = tensor.size()\n",
    "    features = tensor.view(b * c, h * w)\n",
    "    gram = torch.mm(features, features.t())\n",
    "    return gram / (b * c * h * w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from model layers\n",
    "def get_features(image, model, layers_dict):\n",
    "    \"\"\"Extract activation maps from selected layers in the model\"\"\"\n",
    "    features = {}\n",
    "    x = image\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers_dict:\n",
    "            features[layers_dict[name]] = x\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_style_transfer(style_loss_network, content_image, style_image, \n",
    "                          content_layers, style_layers, \n",
    "                          content_weight, style_weight, \n",
    "                          style_layer_weights, num_steps=300):\n",
    "    \"\"\"\n",
    "    Generic neural style transfer function\n",
    "    \n",
    "    Args:\n",
    "        model: Pre-trained network (VGG-19 or AlexNet features)\n",
    "        content_image: Image whose structure to preserve\n",
    "        style_image: Image whose artistic style to apply\n",
    "        content_layers: Dict mapping layer indices to names for content features\n",
    "        style_layers: Dict mapping layer indices to names for style features\n",
    "        content_weight: Scalar (\u03b1) to control emphasis on content\n",
    "        style_weight: Scalar (\u03b2) to control intensity of style\n",
    "        style_layer_weights: Dict of weights to balance each style layer contribution\n",
    "        num_steps: Number of optimization steps\n",
    "    \n",
    "    Returns:\n",
    "        Final stylized image tensor\n",
    "    \"\"\"\n",
    "    # Set model to eval mode and freeze parameters\n",
    "    style_loss_network.eval()\n",
    "    for param in style_loss_network.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Initialize target image as clone of content image\n",
    "    target = content_image.clone().requires_grad_(True)\n",
    "    \n",
    "    # Combine all layers needed\n",
    "    all_layers = {**content_layers, **style_layers}\n",
    "    \n",
    "    # Extract fixed features from source images\n",
    "    content_features = get_features(content_image, style_loss_network, content_layers)\n",
    "    style_features = get_features(style_image, style_loss_network, style_layers)\n",
    "    \n",
    "    # Compute Gram matrices for style image\n",
    "    style_grams = {layer: get_gram_matrix(style_features[layer]) for layer in style_features}\n",
    "\n",
    "    # Setup optimizer (LBFGS recommended for NST)\n",
    "    optimizer = optim.LBFGS([target])\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(num_steps):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Extract current features from target\n",
    "            target_features = get_features(target, model, all_layers)\n",
    "            \n",
    "            # Compute Content Loss\n",
    "            c_loss = 0\n",
    "            for layer in content_layers.values():\n",
    "                c_loss += torch.mean((target_features[layer] - content_features[layer])**2)\n",
    "            \n",
    "            # Compute Style Loss\n",
    "            s_loss = 0\n",
    "            for layer in style_layers.values():\n",
    "                target_gram = get_gram_matrix(target_features[layer])\n",
    "                style_gram = style_grams[layer]\n",
    "                layer_weight = style_layer_weights.get(layer, 1.0)\n",
    "                s_loss += layer_weight * torch.mean((target_gram - style_gram)**2)\n",
    "            \n",
    "            # Total weighted loss\n",
    "            total_loss = content_weight * c_loss + style_weight * s_loss\n",
    "            total_loss.backward()\n",
    "            \n",
    "            return total_loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "        \n",
    "        # Optional: print progress every 50 steps\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Step {i+1}/{num_steps} completed\")\n",
    "    \n",
    "    return target.detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Style and Content Images\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Van Gogh paintings to use as style images\n",
    "# We need at least 5 different Van Gogh paintings\n",
    "\n",
    "def find_van_gogh_paintings(base_dir, min_count=5):\n",
    "    \"\"\"Find Van Gogh painting files\"\"\"\n",
    "    van_gogh_files = []\n",
    "    \n",
    "    # Search for specific famous paintings\n",
    "    keywords = [\n",
    "        \"starry-night\",\n",
    "        \"sunflowers\",\n",
    "        \"crows\",  # Wheatfield with Crows\n",
    "        \"cafe-terrace\",\n",
    "        \"irises\",\n",
    "        \"bedroom\",\n",
    "        \"almond-blossom\",\n",
    "        \"self-portrait\"\n",
    "    ]\n",
    "    \n",
    "    for fname in os.listdir(base_dir):\n",
    "        if not fname.lower().endswith((\".jpg\", \".png\")):\n",
    "            continue\n",
    "        \n",
    "        if \"van-gogh\" in fname.lower():\n",
    "            # Check if it matches any keyword\n",
    "            for keyword in keywords:\n",
    "                if keyword in fname.lower():\n",
    "                    filepath = os.path.join(base_dir, fname)\n",
    "                    if filepath not in van_gogh_files:\n",
    "                        van_gogh_files.append(filepath)\n",
    "                    break\n",
    "    \n",
    "    # If we don't have enough, add more random Van Gogh paintings\n",
    "    if len(van_gogh_files) < min_count:\n",
    "        for fname in os.listdir(base_dir):\n",
    "            if len(van_gogh_files) >= min_count:\n",
    "                break\n",
    "            if not fname.lower().endswith((\".jpg\", \".png\")):\n",
    "                continue\n",
    "            if \"van-gogh\" in fname.lower():\n",
    "                filepath = os.path.join(base_dir, fname)\n",
    "                if filepath not in van_gogh_files:\n",
    "                    van_gogh_files.append(filepath)\n",
    "    \n",
    "    return van_gogh_files[:min_count] if len(van_gogh_files) >= min_count else van_gogh_files\n",
    "\n",
    "# Find base directory\n",
    "possible_dirs = [\n",
    "    \"/kaggle/input/wikiart/Post_Impressionism\",\n",
    "    \"/kaggle/input/Post_Impressionism\",\n",
    "    \"/content/data/Post_Impressionism\",\n",
    "    \"./Post_Impressionism\"\n",
    "]\n",
    "\n",
    "base_dir = None\n",
    "for dir_path in possible_dirs:\n",
    "    if os.path.exists(dir_path):\n",
    "        base_dir = dir_path\n",
    "        break\n",
    "\n",
    "if base_dir:\n",
    "    style_image_paths = find_van_gogh_paintings(base_dir, min_count=5)\n",
    "    print(f\"Found {len(style_image_paths)} Van Gogh style images:\")\n",
    "    for i, path in enumerate(style_image_paths, 1):\n",
    "        print(f\"  {i}. {os.path.basename(path)}\")\n",
    "else:\n",
    "    print(\"Warning: Could not find Post_Impressionism directory\")\n",
    "    print(\"Please manually specify style_image_paths below\")\n",
    "    style_image_paths = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually specify style images if automatic search didn't work\n",
    "# Or add your preferred Van Gogh paintings here\n",
    "if len(style_image_paths) < 5:\n",
    "    # Example paths (adjust based on your setup)\n",
    "    style_image_paths = [\n",
    "        \"/kaggle/input/wikiart/Post_Impressionism/vincent-van-gogh_the-starry-night-1889(1).jpg\",\n",
    "        \"/kaggle/input/wikiart/Post_Impressionism/vincent-van-gogh_sunflowers-1888.jpg\",\n",
    "        \"/kaggle/input/wikiart/Post_Impressionism/vincent-van-gogh_wheatfield-with-crows-1890.jpg\",\n",
    "        \"/kaggle/input/wikiart/Post_Impressionism/vincent-van-gogh_cafe-terrace-at-night-1888.jpg\",\n",
    "        \"/kaggle/input/wikiart/Post_Impressionism/vincent-van-gogh_irises-1889.jpg\"\n",
    "    ]\n",
    "    \n",
    "    # Filter to only existing files\n",
    "    style_image_paths = [p for p in style_image_paths if os.path.exists(p)]\n",
    "    print(f\"Using {len(style_image_paths)} manually specified style images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare content images\n",
    "# You should provide 20 content images (personal photos are encouraged!)\n",
    "# For now, we'll create a placeholder that you can replace\n",
    "\n",
    "def find_content_images(content_dir=None, count=20):\n",
    "    \"\"\"Find content images for style transfer\"\"\"\n",
    "    content_paths = []\n",
    "    \n",
    "    # Try to find content images directory\n",
    "    possible_dirs = [\n",
    "        \"/kaggle/input/content\",\n",
    "        \"/kaggle/input/content-images\",\n",
    "        \"./content_images\",\n",
    "        content_dir\n",
    "    ]\n",
    "    \n",
    "    content_base = None\n",
    "    for dir_path in possible_dirs:\n",
    "        if dir_path and os.path.exists(dir_path):\n",
    "            content_base = dir_path\n",
    "            break\n",
    "    \n",
    "    if content_base:\n",
    "        # Load all images from directory\n",
    "        for fname in os.listdir(content_base):\n",
    "            if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                content_paths.append(os.path.join(content_base, fname))\n",
    "                if len(content_paths) >= count:\n",
    "                    break\n",
    "    \n",
    "    return content_paths\n",
    "\n",
    "content_image_paths = find_content_images(count=20)\n",
    "\n",
    "if len(content_image_paths) < 20:\n",
    "    print(f\"Warning: Found only {len(content_image_paths)} content images\")\n",
    "    print(\"Please add more content images or use images from the dataset\")\n",
    "    \n",
    "    # Fallback: use some non-Van-Gogh paintings as content images\n",
    "    if base_dir:\n",
    "        print(\"Using non-Van-Gogh paintings as content images...\")\n",
    "        for fname in os.listdir(base_dir):\n",
    "            if len(content_image_paths) >= 20:\n",
    "                break\n",
    "            if not fname.lower().endswith((\".jpg\", \".png\")):\n",
    "                continue\n",
    "            if \"van-gogh\" not in fname.lower():\n",
    "                content_image_paths.append(os.path.join(base_dir, fname))\n",
    "\n",
    "print(f\"\\nPrepared {len(content_image_paths)} content images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Layer Configurations for VGG-19 and AlexNet\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG-19 layer configuration\n",
    "VGG19_CONTENT_LAYERS = {'21': 'content'}  # conv4_2\n",
    "\n",
    "VGG19_STYLE_LAYERS = {\n",
    "    '0': 'conv1_1',\n",
    "    '5': 'conv2_1',\n",
    "    '10': 'conv3_1',\n",
    "    '19': 'conv4_1',\n",
    "    '28': 'conv5_1'\n",
    "}\n",
    "\n",
    "VGG19_STYLE_LAYER_WEIGHTS = {\n",
    "    'conv1_1': 1.0,\n",
    "    'conv2_1': 0.8,\n",
    "    'conv3_1': 0.5,\n",
    "    'conv4_1': 0.3,\n",
    "    'conv5_1': 0.1\n",
    "}\n",
    "\n",
    "# AlexNet layer configuration (simpler, fewer layers)\n",
    "ALEXNET_CONTENT_LAYERS = {'8': 'content'}  # conv3\n",
    "\n",
    "ALEXNET_STYLE_LAYERS = {\n",
    "    '0': 'conv1',\n",
    "    '3': 'conv2',\n",
    "    '6': 'conv3'\n",
    "}\n",
    "\n",
    "ALEXNET_STYLE_LAYER_WEIGHTS = {\n",
    "    'conv1': 1.0,\n",
    "    'conv2': 0.75,\n",
    "    'conv3': 0.5\n",
    "}\n",
    "\n",
    "print(\"Layer configurations defined for VGG-19 and AlexNet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Part 2B: Hyperparameter Search with Optuna\n",
    "\n",
    "Find optimal style transfer hyperparameters using the Part 1 classifier as a judge.\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Weights & Biases\n",
    "wandb.login(key=\"16d1bc863b28f81253ac0ee253b453393791a7e1\")\n",
    "print(\"Logged in to Weights & Biases\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG-19 features model for style transfer\n",
    "vgg_features = models.vgg19(weights='IMAGENET1K_V1').features.to(device).eval()\n",
    "for param in vgg_features.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"VGG-19 features model loaded for style transfer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select fixed content and style images for hyperparameter search\n",
    "# Using one content image and one style image for consistency\n",
    "if len(content_image_paths) > 0 and len(style_image_paths) > 0:\n",
    "    search_content_path = content_image_paths[0]\n",
    "    search_style_path = style_image_paths[0]  # Use first Van Gogh painting\n",
    "    \n",
    "    search_content_img = image_loader(search_content_path)\n",
    "    search_style_img = image_loader(search_style_path)\n",
    "    \n",
    "    print(f\"Using content image: {os.path.basename(search_content_path)}\")\n",
    "    print(f\"Using style image: {os.path.basename(search_style_path)}\")\n",
    "    \n",
    "    # Display the images\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    imshow(search_content_img, \"Content Image\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    imshow(search_style_img, \"Style Image (Van Gogh)\")\n",
    "else:\n",
    "    raise ValueError(\"Need at least one content and one style image for hyperparameter search!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer_objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for style transfer hyperparameter search.\n",
    "    Maximizes the classifier's probability that the stylized image is Van Gogh.\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    content_weight = trial.suggest_float(\"content_weight\", 1e-4, 1e2, log=True)\n",
    "    style_weight = trial.suggest_float(\"style_weight\", 1e3, 1e9, log=True)\n",
    "    \n",
    "    # Optional: also search style_layer_weights (can be commented out for faster search)\n",
    "    # For now, use fixed style_layer_weights to reduce search space\n",
    "    \n",
    "    # Run style transfer with suggested parameters\n",
    "    # Use fewer steps for faster search (can increase for final results)\n",
    "    num_steps = 100  # Reduced for faster hyperparameter search\n",
    "    \n",
    "    try:\n",
    "        stylized_image = neural_style_transfer(\n",
    "            vgg_features,\n",
    "            search_content_img,\n",
    "            search_style_img,\n",
    "            VGG19_CONTENT_LAYERS,\n",
    "            VGG19_STYLE_LAYERS,\n",
    "            content_weight,\n",
    "            style_weight,\n",
    "            VGG19_STYLE_LAYER_WEIGHTS,\n",
    "            num_steps=num_steps\n",
    "        )\n",
    "        \n",
    "        # Prepare image for classifier (resize to 224x224)\n",
    "        judge_input = F.interpolate(stylized_image, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Get score from judge (classifier)\n",
    "        with torch.no_grad():\n",
    "            output = judge_model(judge_input)\n",
    "            probabilities = F.softmax(output, dim=1)\n",
    "            # Assuming class 1 is \"Van Gogh\" (check your model's class order)\n",
    "            van_gogh_prob = probabilities[0][1].item()  # Probability of being Van Gogh\n",
    "        \n",
    "        return van_gogh_prob\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in trial {trial.number}: {e}\")\n",
    "        return 0.0  # Return low score on error\n",
    "\n",
    "print(\"Style transfer objective function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna hyperparameter search\n",
    "print(\"=\"*60)\n",
    "print(\"PART 2B: HYPERPARAMETER SEARCH FOR STYLE TRANSFER\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nObjective: Maximize classifier's Van Gogh probability\")\n",
    "print(\"This may take 30-60 minutes depending on number of trials...\")\n",
    "\n",
    "# Create study\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=2)\n",
    ")\n",
    "\n",
    "# Initialize wandb run for tracking\n",
    "wandb_run = wandb.init(\n",
    "    project=\"VanGogh_StyleTransfer_HPO\",\n",
    "    name=\"style_transfer_hyperparameter_search\",\n",
    "    config={\"judge_model\": judge_model_name}\n",
    ")\n",
    "\n",
    "# Track start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Run optimization (adjust n_trials and timeout as needed)\n",
    "# For faster results, use fewer trials; for better results, use more\n",
    "study.optimize(style_transfer_objective, n_trials=15, timeout=3600, show_progress_bar=True)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_minutes = elapsed_time / 60\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYPERPARAMETER SEARCH COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTime taken: {elapsed_minutes:.2f} minutes ({elapsed_time:.0f} seconds)\")\n",
    "print(f\"Completed trials: {len(study.trials)}\")\n",
    "print(f\"\\nBest Van Gogh probability: {study.best_value:.4f}\")\n",
    "print(f\"\\nOptimal hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Log best parameters to wandb\n",
    "wandb.log({\"best_van_gogh_prob\": study.best_value, **study.best_params})\n",
    "wandb_run.finish()\n",
    "\n",
    "# Save results\n",
    "best_style_transfer_params = study.best_params\n",
    "with open('best_style_transfer_params.pkl', 'wb') as f:\n",
    "    pickle.dump(best_style_transfer_params, f)\n",
    "print(\"\\nSaved optimal parameters to 'best_style_transfer_params.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Part 2C: Application & Evaluation\n",
    "\n",
    "Apply style transfer to 20 images using both VGG-19 and AlexNet, then evaluate with both classifiers.\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimal hyperparameters (or use defaults if search wasn't run)\n",
    "if 'best_style_transfer_params' not in globals():\n",
    "    if os.path.exists('best_style_transfer_params.pkl'):\n",
    "        with open('best_style_transfer_params.pkl', 'rb') as f:\n",
    "            best_style_transfer_params = pickle.load(f)\n",
    "        print(f\"Loaded optimal parameters from file\")\n",
    "    else:\n",
    "        # Use reasonable defaults if search wasn't run\n",
    "        best_style_transfer_params = {\n",
    "            'content_weight': 1e-2,\n",
    "            'style_weight': 1e6\n",
    "        }\n",
    "        print(\"Using default parameters (hyperparameter search not run)\")\n",
    "\n",
    "print(f\"\\nUsing hyperparameters:\")\n",
    "for k, v in best_style_transfer_params.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both VGG-19 and AlexNet feature models\n",
    "vgg19_features = models.vgg19(weights='IMAGENET1K_V1').features.to(device).eval()\n",
    "for param in vgg19_features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "alexnet_features = models.alexnet(weights='IMAGENET1K_V1').features.to(device).eval()\n",
    "for param in alexnet_features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Both VGG-19 and AlexNet feature models loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set consistent number of epochs/steps for all style transfers\n",
    "# This must be the same for all images and models (project requirement)\n",
    "CONSISTENT_NUM_STEPS = 300  # You can adjust this, but keep it consistent\n",
    "\n",
    "print(f\"Using {CONSISTENT_NUM_STEPS} optimization steps for all style transfers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply style transfer and evaluate\n",
    "def apply_and_evaluate_style_transfer(content_path, style_path, model_name='VGG19', \n",
    "                                     save_dir='/kaggle/working/stylized_images'):\n",
    "    \"\"\"\n",
    "    Apply style transfer and evaluate with both classifiers\n",
    "    \n",
    "    Returns:\n",
    "        dict with stylized image, evaluation scores, and metadata\n",
    "    \"\"\"\n",
    "    # Load images\n",
    "    content_img = image_loader(content_path)\n",
    "    style_img = image_loader(style_path)\n",
    "    \n",
    "    # Select model and layer configuration\n",
    "    if model_name == 'VGG19':\n",
    "        style_loss_network = vgg19_features\n",
    "        content_layers = VGG19_CONTENT_LAYERS\n",
    "        style_layers = VGG19_STYLE_LAYERS\n",
    "        style_layer_weights = VGG19_STYLE_LAYER_WEIGHTS\n",
    "    elif model_name == 'AlexNet':\n",
    "        style_loss_network = alexnet_features\n",
    "        content_layers = ALEXNET_CONTENT_LAYERS\n",
    "        style_layers = ALEXNET_STYLE_LAYERS\n",
    "        style_layer_weights = ALEXNET_STYLE_LAYER_WEIGHTS\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    # Apply style transfer\n",
    "    print(f\"\\nApplying {model_name} style transfer...\")\n",
    "    stylized = neural_style_transfer(\n",
    "        style_loss_network, content_img, style_img,\n",
    "        content_layers, style_layers,\n",
    "        best_style_transfer_params['content_weight'],\n",
    "        best_style_transfer_params['style_weight'],\n",
    "        style_layer_weights,\n",
    "        num_steps=CONSISTENT_NUM_STEPS\n",
    "    )\n",
    "    \n",
    "    # Prepare for classifier evaluation\n",
    "    eval_input = F.interpolate(stylized, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    \n",
    "    # Evaluate with judge_model (and potentially other classifiers)\n",
    "    with torch.no_grad():\n",
    "        output = judge_model(eval_input)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        judge_van_gogh_prob = probs[0][1].item()\n",
    "        judge_prediction = output.argmax(dim=1).item()\n",
    "    \n",
    "    # Save stylized image\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    content_name = os.path.splitext(os.path.basename(content_path))[0]\n",
    "    style_name = os.path.splitext(os.path.basename(style_path))[0]\n",
    "    save_path = os.path.join(save_dir, f\"{model_name}_{content_name}_{style_name}.jpg\")\n",
    "    \n",
    "    # Convert and save\n",
    "    result_image = imshow(stylized, f\"{model_name}: {os.path.basename(content_path)}\", save_path=save_path)\n",
    "    \n",
    "    return {\n",
    "        'stylized_image': stylized,\n",
    "        'content_path': content_path,\n",
    "        'style_path': style_path,\n",
    "        'model_name': model_name,\n",
    "        'judge_van_gogh_prob': judge_van_gogh_prob,\n",
    "        'judge_prediction': judge_prediction,\n",
    "        'save_path': save_path\n",
    "    }\n",
    "\n",
    "print(\"Style transfer application function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply style transfer to 20 images with both VGG-19 and AlexNet\n",
    "# Use at least 5 different Van Gogh paintings as style images\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PART 2C: APPLYING STYLE TRANSFER TO 20 IMAGES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ensure we have enough content and style images\n",
    "num_content = min(20, len(content_image_paths))\n",
    "num_styles = min(5, len(style_image_paths))\n",
    "\n",
    "print(f\"\\nProcessing {num_content} content images with {num_styles} style images\")\n",
    "print(f\"Using {CONSISTENT_NUM_STEPS} steps for all transfers\")\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "# Process each content image with each style image, using both models\n",
    "content_to_process = content_image_paths[:num_content]\n",
    "styles_to_use = style_image_paths[:num_styles]\n",
    "\n",
    "# Distribute style images across content images\n",
    "# Each content image gets styled with different Van Gogh paintings\n",
    "for i, content_path in enumerate(tqdm(content_to_process, desc=\"Processing content images\")):\n",
    "    # Cycle through style images\n",
    "    style_idx = i % num_styles\n",
    "    style_path = styles_to_use[style_idx]\n",
    "    \n",
    "    # Apply with VGG-19\n",
    "    try:\n",
    "        vgg_result = apply_and_evaluate_style_transfer(\n",
    "            content_path, style_path, model_name='VGG19'\n",
    "        )\n",
    "        all_results.append(vgg_result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with VGG-19 on {content_path}: {e}\")\n",
    "    \n",
    "    # Apply with AlexNet\n",
    "    try:\n",
    "        alexnet_result = apply_and_evaluate_style_transfer(\n",
    "            content_path, style_path, model_name='AlexNet'\n",
    "        )\n",
    "        all_results.append(alexnet_result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with AlexNet on {content_path}: {e}\")\n",
    "\n",
    "print(f\"\\nCompleted {len(all_results)} style transfers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate results with judge_model (trained_classifier from Part A)\n",
    "# We use the same judge_model for all evaluations\n",
    "\n",
    "def evaluate_with_classifier(stylized_image, classifier, classifier_name):\n",
    "    \"\"\"Evaluate a stylized image with a classifier\"\"\"\n",
    "    eval_input = F.interpolate(stylized_image, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = classifier(eval_input)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        van_gogh_prob = probs[0][1].item()\n",
    "        prediction = output.argmax(dim=1).item()\n",
    "    \n",
    "    return {\n",
    "        f'{classifier_name}_van_gogh_prob': van_gogh_prob,\n",
    "        f'{classifier_name}_prediction': prediction\n",
    "    }\n",
    "\n",
    "# Use judge_model (trained_classifier from Part A) for all evaluations\n",
    "print(\"\\nEvaluating all stylized images with judge_model (trained_classifier from Part A)...\")\n",
    "\n",
    "for result in tqdm(all_results, desc=\"Evaluating\"):\n",
    "    stylized = result['stylized_image']\n",
    "    \n",
    "    # Evaluate with judge model (already done, but add to results)\n",
    "    result['judge_eval'] = {\n",
    "        'van_gogh_prob': result['judge_van_gogh_prob'],\n",
    "        'prediction': result['judge_prediction']\n",
    "    }\n",
    "    \n",
    "    # All evaluations use the same judge_model (trained_classifier from Part A)\n",
    "    # This is the trained classifier from Part A\n",
    "\n",
    "print(\"\\nEvaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and summarize results\n",
    "print(\"=\"*60)\n",
    "print(\"PART 2C: RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'content': os.path.basename(r['content_path']),\n",
    "        'style': os.path.basename(r['style_path']),\n",
    "        'model': r['model_name'],\n",
    "        'judge_van_gogh_prob': r['judge_van_gogh_prob'],\n",
    "        'judge_prediction': r['judge_prediction']\n",
    "    }\n",
    "    for r in all_results\n",
    "])\n",
    "\n",
    "# Add classifier evaluations if available\n",
    "for r in all_results:\n",
    "    if 'VGG19_classifier_van_gogh_prob' in r:\n",
    "        idx = results_df[results_df['content'] == os.path.basename(r['content_path'])].index[0]\n",
    "        results_df.loc[idx, 'VGG19_classifier_van_gogh_prob'] = r.get('VGG19_classifier_van_gogh_prob', None)\n",
    "    if 'AlexNet_classifier_van_gogh_prob' in r:\n",
    "        idx = results_df[results_df['content'] == os.path.basename(r['content_path'])].index[0]\n",
    "        results_df.loc[idx, 'AlexNet_classifier_van_gogh_prob'] = r.get('AlexNet_classifier_van_gogh_prob', None)\n",
    "\n",
    "print(\"\\nResults Summary:\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "# Statistics by model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICS BY STYLE TRANSFER MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model in ['VGG19', 'AlexNet']:\n",
    "    model_results = results_df[results_df['model'] == model]\n",
    "    if len(model_results) > 0:\n",
    "        print(f\"\\n{model} Style Transfer:\")\n",
    "        print(f\"  Number of images: {len(model_results)}\")\n",
    "        print(f\"  Mean Van Gogh probability (Judge): {model_results['judge_van_gogh_prob'].mean():.4f}\")\n",
    "        print(f\"  Std Van Gogh probability (Judge): {model_results['judge_van_gogh_prob'].std():.4f}\")\n",
    "        print(f\"  Predicted as Van Gogh: {model_results['judge_prediction'].sum()}/{len(model_results)}\")\n",
    "        \n",
    "        if 'VGG19_classifier_van_gogh_prob' in model_results.columns:\n",
    "            vgg_probs = model_results['VGG19_classifier_van_gogh_prob'].dropna()\n",
    "            if len(vgg_probs) > 0:\n",
    "                print(f\"  Mean Van Gogh probability (VGG-19 Classifier): {vgg_probs.mean():.4f}\")\n",
    "        \n",
    "        if 'AlexNet_classifier_van_gogh_prob' in model_results.columns:\n",
    "            alexnet_probs = model_results['AlexNet_classifier_van_gogh_prob'].dropna()\n",
    "            if len(alexnet_probs) > 0:\n",
    "                print(f\"  Mean Van Gogh probability (AlexNet Classifier): {alexnet_probs.mean():.4f}\")\n",
    "\n",
    "# Comparison between VGG-19 and AlexNet\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VGG-19 vs ALEXNET COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vgg19_mean = results_df[results_df['model'] == 'VGG19']['judge_van_gogh_prob'].mean()\n",
    "alexnet_mean = results_df[results_df['model'] == 'AlexNet']['judge_van_gogh_prob'].mean()\n",
    "\n",
    "print(f\"\\nVGG-19 mean Van Gogh probability: {vgg19_mean:.4f}\")\n",
    "print(f\"AlexNet mean Van Gogh probability: {alexnet_mean:.4f}\")\n",
    "print(f\"Difference: {abs(vgg19_mean - alexnet_mean):.4f}\")\n",
    "\n",
    "if vgg19_mean > alexnet_mean:\n",
    "    print(\"\\nVGG-19 produces more 'Van Gogh-like' results according to the classifier\")\n",
    "else:\n",
    "    print(\"\\nAlexNet produces more 'Van Gogh-like' results according to the classifier\")\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('/kaggle/working/style_transfer_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'style_transfer_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some example results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE STYLIZED IMAGES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show top 6 results by Van Gogh probability\n",
    "top_results = sorted(all_results, key=lambda x: x['judge_van_gogh_prob'], reverse=True)[:6]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Top 6 Stylized Images (by Van Gogh Probability)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, result in enumerate(top_results):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    \n",
    "    # Display image\n",
    "    image = result['stylized_image'].cpu().clone().detach().squeeze(0)\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    )\n",
    "    image = inv_normalize(image)\n",
    "    image = transforms.ToPILImage()(image.clamp(0, 1))\n",
    "    \n",
    "    axes[row, col].imshow(image)\n",
    "    title = f\"{result['model_name']}\\nProb: {result['judge_van_gogh_prob']:.3f}\"\n",
    "    axes[row, col].set_title(title, fontsize=10)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/top_stylized_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: top_stylized_images.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"PART B - FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nPart 2B: Hyperparameter Search\")\n",
    "print(f\"  Optimal content_weight: {best_style_transfer_params.get('content_weight', 'N/A')}\")\n",
    "print(f\"  Optimal style_weight: {best_style_transfer_params.get('style_weight', 'N/A')}\")\n",
    "if 'study' in globals():\n",
    "    print(f\"  Best Van Gogh probability achieved: {study.best_value:.4f}\")\n",
    "\n",
    "print(\"\\nPart 2C: Application & Evaluation\")\n",
    "if 'all_results' in globals():\n",
    "    print(f\"  Total style transfers completed: {len(all_results)}\")\n",
    "if 'num_content' in globals():\n",
    "    print(f\"  Content images used: {num_content}\")\n",
    "if 'num_styles' in globals():\n",
    "    print(f\"  Style images used: {num_styles}\")\n",
    "if 'CONSISTENT_NUM_STEPS' in globals():\n",
    "    print(f\"  Optimization steps per image: {CONSISTENT_NUM_STEPS}\")\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "if 'vgg19_mean' in globals() and 'alexnet_mean' in globals():\n",
    "    print(f\"  VGG-19 mean probability: {vgg19_mean:.4f}\")\n",
    "    print(f\"  AlexNet mean probability: {alexnet_mean:.4f}\")\n",
    "else:\n",
    "    print(\"  Run results analysis cell to see comparison\")\n",
    "\n",
    "print(\"\\nSaved Files:\")\n",
    "print(\"  - best_style_transfer_params.pkl\")\n",
    "print(\"  - style_transfer_results.csv\")\n",
    "print(\"  - top_stylized_images.png\")\n",
    "print(\"  - stylized_images/ (directory with all stylized images)\")\n",
    "\n",
    "print(\"\\nPart B Complete!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INTEGRATION: Classify Final Stylized Image with Trained Classifier\n",
    "# ============================================================\n",
    "\n",
    "# Get the final stylized image from Part B (use the last result)\n",
    "if \"all_results\" in globals() and len(all_results) > 0:\n",
    "    # Get the last stylized image (or you can select a specific one)\n",
    "    final_stylized_image = all_results[-1]['stylized_image']\n",
    "    \n",
    "    print(\"Classifying final stylized image with trained_classifier from Part A...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Preprocess the stylized image for the classifier\n",
    "    # Resize to (224, 224) and normalize with ImageNet stats\n",
    "    classifier_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # The image is already a tensor, so we need to handle it differently\n",
    "    # Convert from (1, 3, H, W) format, resize, then normalize\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    # Resize to 224x224\n",
    "    preprocessed_image = F.interpolate(\n",
    "        final_stylized_image,\n",
    "        size=(224, 224),\n",
    "        mode='bilinear',\n",
    "        align_corners=False\n",
    "    )\n",
    "    \n",
    "    # Normalize (the image should already be normalized from style transfer, but ensure it matches classifier input)\n",
    "    # ImageNet normalization\n",
    "    IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
    "    IMAGENET_STD = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "    \n",
    "    # Denormalize first (in case it was normalized), then re-normalize\n",
    "    # Check if it needs normalization - style transfer output should be in [0,1] range\n",
    "    # We need to normalize it for the classifier\n",
    "    # First, ensure it is in [0,1] range, then apply ImageNet normalization\n",
    "    \n",
    "    # Denormalize from style transfer normalization (if applied)\n",
    "    # Style transfer uses ImageNet normalization, so we reverse it first\n",
    "    style_mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
    "    style_std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "    \n",
    "    # Reverse normalization: (x * std) + mean\n",
    "    denormalized = preprocessed_image * style_std + style_mean\n",
    "    \n",
    "    # Clamp to [0, 1] range\n",
    "    denormalized = torch.clamp(denormalized, 0, 1)\n",
    "    \n",
    "    # Now normalize for classifier: (x - mean) / std\n",
    "    classifier_input = (denormalized - IMAGENET_MEAN) / IMAGENET_STD\n",
    "    \n",
    "    # Set classifier to eval mode\n",
    "    trained_classifier.eval()\n",
    "    \n",
    "    # Classify the image\n",
    "    with torch.no_grad():\n",
    "        output = trained_classifier(classifier_input)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        van_gogh_prob = probabilities[0][1].item()  # Probability of being Van Gogh (class 1)\n",
    "        not_van_gogh_prob = probabilities[0][0].item()  # Probability of not being Van Gogh (class 0)\n",
    "        prediction = output.argmax(dim=1).item()\n",
    "    \n",
    "    print(f\"\\nClassification Results:\")\n",
    "    print(f\"  Probability of being Van Gogh: {van_gogh_prob:.4f} ({van_gogh_prob*100:.2f}%)\")\n",
    "    print(f\"  Probability of NOT being Van Gogh: {not_van_gogh_prob:.4f} ({not_van_gogh_prob*100:.2f}%)\")\n",
    "    print(f\"  Prediction: {'Van Gogh' if prediction == 1 else 'Not Van Gogh'}\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"No stylized images found. Please run Part B style transfer first.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2477766,
     "sourceId": 4202543,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31240,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}